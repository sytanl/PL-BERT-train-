{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc4ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/BERT/lib/python3.8/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import LoggerType\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import AlbertConfig, AlbertModel\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from model import MultiTaskModel\n",
    "from dataloader import build_dataloader\n",
    "from utils import length_to_mask, scan_checkpoint\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d0c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pickle\n",
    "\n",
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a7f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(config['dataset_params']['token_maps'], 'rb') as handle:\n",
    "    token_maps = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158bf338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/BERT/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e60819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # F0 loss (regression)\n",
    "\n",
    "best_loss = float('inf')  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "loss_train_record = list([])\n",
    "loss_test_record = list([])\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "log_interval = config['log_interval']\n",
    "save_interval = config['save_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4fa9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    \n",
    "    curr_steps = 0\n",
    "    \n",
    "    dataset = load_from_disk(config[\"data_folder\"])\n",
    "\n",
    "    log_dir = config['log_dir']\n",
    "    if not osp.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n",
    "    shutil.copy(config_path, osp.join(log_dir, osp.basename(config_path)))\n",
    "    \n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = build_dataloader(dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    num_workers=0, \n",
    "                                    dataset_config=config['dataset_params'])\n",
    "\n",
    "    albert_base_configuration = AlbertConfig(**config['model_params'])\n",
    "    \n",
    "    bert = AlbertModel(albert_base_configuration)\n",
    "    bert = MultiTaskModel(bert, \n",
    "                          num_vocab=1 + max([m['token'] for m in token_maps.values()]), \n",
    "                          num_tokens=config['model_params']['vocab_size'],\n",
    "                          hidden_size=config['model_params']['hidden_size'])\n",
    "    \n",
    "    load = True\n",
    "    try:\n",
    "        files = os.listdir(log_dir)\n",
    "        ckpts = []\n",
    "        for f in os.listdir(log_dir):\n",
    "            if f.startswith(\"step_\"): ckpts.append(f)\n",
    "\n",
    "        iters = [int(f.split('_')[-1].split('.')[0]) for f in ckpts if os.path.isfile(os.path.join(log_dir, f))]\n",
    "        iters = sorted(iters)[-1]\n",
    "    except:\n",
    "        iters = 0\n",
    "        load = False\n",
    "    \n",
    "    optimizer = AdamW(bert.parameters(), lr=1e-4)\n",
    "    \n",
    "    accelerator = Accelerator(mixed_precision=config['mixed_precision'], split_batches=True, kwargs_handlers=[ddp_kwargs])\n",
    "    \n",
    "    if load:\n",
    "        checkpoint = torch.load(log_dir + \"/step_\" + str(iters) + \".t7\", map_location='cpu')\n",
    "        state_dict = checkpoint['net']\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "\n",
    "        bert.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        accelerator.print('Checkpoint loaded.')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    bert, optimizer, train_loader = accelerator.prepare(\n",
    "        bert, optimizer, train_loader\n",
    "    )\n",
    "\n",
    "    accelerator.print('Start training...')\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bert.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for _, batch in enumerate(train_loader):        \n",
    "        curr_steps += 1\n",
    "        \n",
    "        words, labels, phonemes, input_lengths, masked_indices = batch\n",
    "        text_mask = length_to_mask(torch.Tensor(input_lengths)).to(device)\n",
    "        \n",
    "        tokens_pred, words_pred = bert(phonemes, attention_mask=(~text_mask).int())\n",
    "        \n",
    "        loss_vocab = 0\n",
    "        for _s2s_pred, _text_input, _text_length, _masked_indices in zip(words_pred, words, input_lengths, masked_indices):\n",
    "            loss_vocab += criterion(_s2s_pred[:_text_length], \n",
    "                                        _text_input[:_text_length])\n",
    "        loss_vocab /= words.size(0)\n",
    "        \n",
    "        loss_token = 0\n",
    "        sizes = 1\n",
    "        for _s2s_pred, _text_input, _text_length, _masked_indices in zip(tokens_pred, labels, input_lengths, masked_indices):\n",
    "            if len(_masked_indices) > 0:\n",
    "                _text_input = _text_input[:_text_length][_masked_indices]\n",
    "                loss_tmp = criterion(_s2s_pred[:_text_length][_masked_indices], \n",
    "                                            _text_input[:_text_length]) \n",
    "                loss_token += loss_tmp\n",
    "                sizes += 1\n",
    "        loss_token /= sizes\n",
    "\n",
    "        loss = loss_vocab + loss_token\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        iters = iters + 1\n",
    "        if (iters+1)%log_interval == 0:\n",
    "            accelerator.print ('Step [%d/%d], Loss: %.5f, Vocab Loss: %.5f, Token Loss: %.5f'\n",
    "                    %(iters+1, num_steps, running_loss / log_interval, loss_vocab, loss_token))\n",
    "            running_loss = 0\n",
    "            \n",
    "        if (iters+1)%save_interval == 0:\n",
    "            accelerator.print('Saving..')\n",
    "\n",
    "            state = {\n",
    "                'net':  bert.state_dict(),\n",
    "                'step': iters,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "            accelerator.save(state, log_dir + '/step_' + str(iters + 1) + '.t7')\n",
    "\n",
    "        if curr_steps > num_steps:\n",
    "            return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c4b8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/BERT/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Step [10/1000000], Loss: 10.52066, Vocab Loss: 7.98611, Token Loss: 2.58173\n",
      "Step [20/1000000], Loss: 9.86193, Vocab Loss: 7.59398, Token Loss: 1.93387\n",
      "Step [30/1000000], Loss: 9.26523, Vocab Loss: 6.64135, Token Loss: 2.04426\n",
      "Step [40/1000000], Loss: 8.54179, Vocab Loss: 6.66423, Token Loss: 1.97085\n",
      "Step [50/1000000], Loss: 8.73489, Vocab Loss: 5.90568, Token Loss: 2.32400\n",
      "Step [60/1000000], Loss: 8.25457, Vocab Loss: 5.99133, Token Loss: 2.26667\n",
      "Step [70/1000000], Loss: 7.88544, Vocab Loss: 6.06705, Token Loss: 2.10967\n",
      "Step [80/1000000], Loss: 7.57618, Vocab Loss: 5.60038, Token Loss: 2.12367\n",
      "Step [90/1000000], Loss: 7.62683, Vocab Loss: 5.74122, Token Loss: 2.14021\n",
      "Step [100/1000000], Loss: 7.69161, Vocab Loss: 5.15269, Token Loss: 2.11090\n",
      "Step [110/1000000], Loss: 7.58778, Vocab Loss: 5.23835, Token Loss: 1.92367\n",
      "Step [120/1000000], Loss: 7.47597, Vocab Loss: 5.99675, Token Loss: 2.16071\n",
      "Step [130/1000000], Loss: 7.39433, Vocab Loss: 5.36668, Token Loss: 1.96671\n",
      "Step [140/1000000], Loss: 7.45030, Vocab Loss: 5.60580, Token Loss: 2.07803\n",
      "Step [150/1000000], Loss: 7.69303, Vocab Loss: 5.46247, Token Loss: 1.92564\n",
      "Step [160/1000000], Loss: 7.60152, Vocab Loss: 5.45534, Token Loss: 2.07374\n",
      "Step [170/1000000], Loss: 7.34862, Vocab Loss: 5.77012, Token Loss: 2.00836\n",
      "Step [180/1000000], Loss: 7.53577, Vocab Loss: 6.26056, Token Loss: 2.13196\n",
      "Step [190/1000000], Loss: 7.42806, Vocab Loss: 6.16873, Token Loss: 1.96505\n",
      "Step [200/1000000], Loss: 7.37527, Vocab Loss: 4.87030, Token Loss: 2.07914\n",
      "Step [210/1000000], Loss: 7.55098, Vocab Loss: 5.44584, Token Loss: 2.13770\n",
      "Step [220/1000000], Loss: 7.38489, Vocab Loss: 5.14625, Token Loss: 1.75968\n",
      "Step [230/1000000], Loss: 7.42641, Vocab Loss: 5.13803, Token Loss: 2.06070\n",
      "Step [240/1000000], Loss: 7.25257, Vocab Loss: 4.85131, Token Loss: 1.89753\n",
      "Step [250/1000000], Loss: 7.07480, Vocab Loss: 5.20923, Token Loss: 2.14979\n",
      "Step [260/1000000], Loss: 7.03624, Vocab Loss: 5.04480, Token Loss: 1.95578\n",
      "Step [270/1000000], Loss: 7.16863, Vocab Loss: 5.08106, Token Loss: 2.24509\n",
      "Step [280/1000000], Loss: 7.32565, Vocab Loss: 5.23414, Token Loss: 1.83411\n",
      "Step [290/1000000], Loss: 7.01734, Vocab Loss: 5.01763, Token Loss: 2.05865\n",
      "Step [300/1000000], Loss: 7.00787, Vocab Loss: 4.36471, Token Loss: 1.82759\n",
      "Step [310/1000000], Loss: 6.71882, Vocab Loss: 4.31993, Token Loss: 2.12765\n",
      "Step [320/1000000], Loss: 6.86921, Vocab Loss: 4.80691, Token Loss: 1.88212\n",
      "Step [330/1000000], Loss: 6.96390, Vocab Loss: 4.45153, Token Loss: 1.74897\n",
      "Step [340/1000000], Loss: 6.75846, Vocab Loss: 5.04583, Token Loss: 2.21453\n",
      "Step [350/1000000], Loss: 6.79467, Vocab Loss: 4.45777, Token Loss: 2.09202\n",
      "Step [360/1000000], Loss: 6.75889, Vocab Loss: 5.70633, Token Loss: 1.90630\n",
      "Step [370/1000000], Loss: 6.20625, Vocab Loss: 3.53396, Token Loss: 1.82376\n",
      "Step [380/1000000], Loss: 6.95888, Vocab Loss: 5.29895, Token Loss: 1.97252\n",
      "Step [390/1000000], Loss: 6.31211, Vocab Loss: 4.88766, Token Loss: 1.72296\n",
      "Step [400/1000000], Loss: 6.48409, Vocab Loss: 4.28495, Token Loss: 1.93638\n",
      "Step [410/1000000], Loss: 6.18241, Vocab Loss: 4.05890, Token Loss: 1.85187\n",
      "Step [420/1000000], Loss: 6.32602, Vocab Loss: 4.31211, Token Loss: 1.99364\n",
      "Step [430/1000000], Loss: 6.41829, Vocab Loss: 4.18959, Token Loss: 1.87334\n",
      "Step [440/1000000], Loss: 6.34046, Vocab Loss: 4.13429, Token Loss: 1.91637\n",
      "Step [450/1000000], Loss: 5.90694, Vocab Loss: 4.54878, Token Loss: 1.59849\n",
      "Step [460/1000000], Loss: 6.05824, Vocab Loss: 4.19206, Token Loss: 2.02278\n",
      "Step [470/1000000], Loss: 6.18660, Vocab Loss: 5.14668, Token Loss: 1.70048\n",
      "Step [480/1000000], Loss: 6.28880, Vocab Loss: 2.99323, Token Loss: 1.96807\n",
      "Step [490/1000000], Loss: 6.53322, Vocab Loss: 3.97491, Token Loss: 1.72897\n",
      "Step [500/1000000], Loss: 6.54832, Vocab Loss: 4.11079, Token Loss: 1.85646\n",
      "Step [510/1000000], Loss: 6.33862, Vocab Loss: 4.23301, Token Loss: 1.51375\n",
      "Step [520/1000000], Loss: 6.22739, Vocab Loss: 4.63315, Token Loss: 1.98661\n",
      "Step [530/1000000], Loss: 6.26615, Vocab Loss: 4.11884, Token Loss: 2.06759\n",
      "Step [540/1000000], Loss: 5.86467, Vocab Loss: 3.38507, Token Loss: 2.06898\n",
      "Step [550/1000000], Loss: 6.13888, Vocab Loss: 3.47327, Token Loss: 2.25935\n",
      "Step [560/1000000], Loss: 5.65672, Vocab Loss: 5.30398, Token Loss: 1.80235\n",
      "Step [570/1000000], Loss: 6.06019, Vocab Loss: 4.53698, Token Loss: 1.97407\n",
      "Step [580/1000000], Loss: 5.70832, Vocab Loss: 3.98925, Token Loss: 1.73511\n",
      "Step [590/1000000], Loss: 6.67045, Vocab Loss: 5.15310, Token Loss: 2.02616\n",
      "Step [600/1000000], Loss: 6.08845, Vocab Loss: 3.21743, Token Loss: 1.89748\n",
      "Step [610/1000000], Loss: 6.55464, Vocab Loss: 3.97860, Token Loss: 2.15218\n",
      "Step [620/1000000], Loss: 5.99696, Vocab Loss: 3.92415, Token Loss: 1.65701\n",
      "Step [630/1000000], Loss: 6.26021, Vocab Loss: 4.06012, Token Loss: 1.97721\n",
      "Step [640/1000000], Loss: 6.15039, Vocab Loss: 4.59856, Token Loss: 1.80835\n",
      "Step [650/1000000], Loss: 5.86442, Vocab Loss: 4.04084, Token Loss: 1.92929\n",
      "Step [660/1000000], Loss: 6.06208, Vocab Loss: 3.00915, Token Loss: 1.86399\n",
      "Step [670/1000000], Loss: 6.14103, Vocab Loss: 3.34025, Token Loss: 1.81922\n",
      "Step [680/1000000], Loss: 6.00394, Vocab Loss: 4.75133, Token Loss: 1.85963\n",
      "Step [690/1000000], Loss: 5.81705, Vocab Loss: 4.97401, Token Loss: 1.81802\n",
      "Step [700/1000000], Loss: 6.13114, Vocab Loss: 3.84911, Token Loss: 2.16639\n",
      "Step [710/1000000], Loss: 5.93584, Vocab Loss: 3.80786, Token Loss: 1.76579\n",
      "Step [720/1000000], Loss: 5.55879, Vocab Loss: 2.61089, Token Loss: 1.87792\n",
      "Step [730/1000000], Loss: 5.90482, Vocab Loss: 4.35101, Token Loss: 1.87748\n",
      "Step [740/1000000], Loss: 5.65796, Vocab Loss: 3.82031, Token Loss: 1.64599\n",
      "Step [750/1000000], Loss: 6.00594, Vocab Loss: 5.04355, Token Loss: 2.02885\n",
      "Step [760/1000000], Loss: 5.83236, Vocab Loss: 3.94922, Token Loss: 1.90203\n",
      "Step [770/1000000], Loss: 6.10189, Vocab Loss: 3.39795, Token Loss: 1.63879\n",
      "Step [780/1000000], Loss: 5.69855, Vocab Loss: 3.77842, Token Loss: 1.56836\n",
      "Step [790/1000000], Loss: 5.88083, Vocab Loss: 3.98722, Token Loss: 2.01758\n",
      "Step [800/1000000], Loss: 5.70223, Vocab Loss: 4.13398, Token Loss: 2.05176\n",
      "Step [810/1000000], Loss: 6.02491, Vocab Loss: 3.57591, Token Loss: 1.97059\n",
      "Step [820/1000000], Loss: 5.61295, Vocab Loss: 3.20607, Token Loss: 1.81784\n",
      "Step [830/1000000], Loss: 5.90066, Vocab Loss: 4.02557, Token Loss: 1.91434\n",
      "Step [840/1000000], Loss: 5.89927, Vocab Loss: 4.14878, Token Loss: 2.86298\n",
      "Step [850/1000000], Loss: 5.85532, Vocab Loss: 4.77735, Token Loss: 2.02819\n",
      "Step [860/1000000], Loss: 5.56117, Vocab Loss: 3.51587, Token Loss: 2.05491\n",
      "Step [870/1000000], Loss: 5.73118, Vocab Loss: 4.06545, Token Loss: 1.77651\n",
      "Step [880/1000000], Loss: 5.69308, Vocab Loss: 3.53118, Token Loss: 1.43781\n",
      "Step [890/1000000], Loss: 5.48810, Vocab Loss: 4.25640, Token Loss: 1.73278\n",
      "Step [900/1000000], Loss: 5.35703, Vocab Loss: 3.77493, Token Loss: 2.03767\n",
      "Step [910/1000000], Loss: 6.00901, Vocab Loss: 3.33541, Token Loss: 1.84198\n",
      "Step [920/1000000], Loss: 5.77491, Vocab Loss: 3.52356, Token Loss: 1.67189\n",
      "Step [930/1000000], Loss: 5.89761, Vocab Loss: 5.00151, Token Loss: 1.98321\n",
      "Step [940/1000000], Loss: 5.77717, Vocab Loss: 5.42781, Token Loss: 1.96014\n",
      "Step [950/1000000], Loss: 5.70281, Vocab Loss: 4.91246, Token Loss: 2.05394\n",
      "Step [960/1000000], Loss: 5.58136, Vocab Loss: 3.61644, Token Loss: 1.82197\n",
      "Step [970/1000000], Loss: 5.46918, Vocab Loss: 3.39974, Token Loss: 1.79633\n",
      "Step [980/1000000], Loss: 6.10485, Vocab Loss: 5.27421, Token Loss: 1.87784\n",
      "Step [990/1000000], Loss: 5.63878, Vocab Loss: 5.13294, Token Loss: 1.82101\n",
      "Step [1000/1000000], Loss: 5.98111, Vocab Loss: 3.73561, Token Loss: 1.53672\n",
      "Step [1010/1000000], Loss: 6.09235, Vocab Loss: 4.70319, Token Loss: 1.80597\n",
      "Step [1020/1000000], Loss: 5.80933, Vocab Loss: 4.17664, Token Loss: 1.58471\n",
      "Step [1030/1000000], Loss: 6.30867, Vocab Loss: 5.07225, Token Loss: 1.93550\n",
      "Step [1040/1000000], Loss: 6.12674, Vocab Loss: 3.94676, Token Loss: 1.84306\n",
      "Step [1050/1000000], Loss: 6.00610, Vocab Loss: 3.98944, Token Loss: 1.86408\n",
      "Step [1060/1000000], Loss: 5.89679, Vocab Loss: 4.52826, Token Loss: 1.81591\n",
      "Step [1070/1000000], Loss: 5.17885, Vocab Loss: 3.39136, Token Loss: 1.85903\n",
      "Step [1080/1000000], Loss: 5.92771, Vocab Loss: 3.90263, Token Loss: 1.86283\n",
      "Step [1090/1000000], Loss: 5.52085, Vocab Loss: 3.54777, Token Loss: 1.86021\n",
      "Step [1100/1000000], Loss: 5.52317, Vocab Loss: 3.97873, Token Loss: 1.98074\n",
      "Step [1110/1000000], Loss: 5.36728, Vocab Loss: 4.24356, Token Loss: 1.76702\n",
      "Step [1120/1000000], Loss: 5.69663, Vocab Loss: 3.51089, Token Loss: 2.06219\n",
      "Step [1130/1000000], Loss: 5.37874, Vocab Loss: 3.10558, Token Loss: 1.78298\n",
      "Step [1140/1000000], Loss: 5.29574, Vocab Loss: 4.03554, Token Loss: 1.97426\n",
      "Step [1150/1000000], Loss: 5.44762, Vocab Loss: 4.25345, Token Loss: 1.92298\n",
      "Step [1160/1000000], Loss: 5.66912, Vocab Loss: 2.53922, Token Loss: 1.88486\n",
      "Step [1170/1000000], Loss: 5.52908, Vocab Loss: 4.80305, Token Loss: 1.83149\n",
      "Step [1180/1000000], Loss: 5.63063, Vocab Loss: 5.95041, Token Loss: 1.79373\n",
      "Step [1190/1000000], Loss: 5.13076, Vocab Loss: 3.81875, Token Loss: 1.75736\n",
      "Step [1200/1000000], Loss: 5.51564, Vocab Loss: 3.45392, Token Loss: 1.82901\n",
      "Step [1210/1000000], Loss: 5.48352, Vocab Loss: 3.67091, Token Loss: 1.69702\n",
      "Step [1220/1000000], Loss: 5.78480, Vocab Loss: 3.76697, Token Loss: 1.74119\n",
      "Step [1230/1000000], Loss: 5.06406, Vocab Loss: 2.30959, Token Loss: 1.73548\n",
      "Step [1240/1000000], Loss: 5.71267, Vocab Loss: 3.37735, Token Loss: 1.78311\n",
      "Step [1250/1000000], Loss: 5.78102, Vocab Loss: 4.51915, Token Loss: 1.82026\n",
      "Step [1260/1000000], Loss: 6.01602, Vocab Loss: 2.79121, Token Loss: 1.71437\n",
      "Step [1270/1000000], Loss: 5.54989, Vocab Loss: 3.48853, Token Loss: 1.62426\n",
      "Step [1280/1000000], Loss: 4.90895, Vocab Loss: 3.55601, Token Loss: 1.83975\n",
      "Step [1290/1000000], Loss: 5.48954, Vocab Loss: 3.08181, Token Loss: 1.65415\n",
      "Step [1300/1000000], Loss: 5.35596, Vocab Loss: 4.57222, Token Loss: 2.00557\n",
      "Step [1310/1000000], Loss: 5.46765, Vocab Loss: 4.12992, Token Loss: 1.82806\n",
      "Step [1320/1000000], Loss: 5.48925, Vocab Loss: 4.53065, Token Loss: 1.76597\n",
      "Step [1330/1000000], Loss: 4.86885, Vocab Loss: 3.24644, Token Loss: 1.21961\n",
      "Step [1340/1000000], Loss: 5.19745, Vocab Loss: 3.58589, Token Loss: 1.74257\n",
      "Step [1350/1000000], Loss: 5.60632, Vocab Loss: 2.59556, Token Loss: 1.74413\n",
      "Step [1360/1000000], Loss: 5.24346, Vocab Loss: 3.26626, Token Loss: 1.75785\n",
      "Step [1370/1000000], Loss: 5.48805, Vocab Loss: 4.08178, Token Loss: 1.76863\n",
      "Step [1380/1000000], Loss: 5.28816, Vocab Loss: 3.47882, Token Loss: 1.77280\n",
      "Step [1390/1000000], Loss: 6.03989, Vocab Loss: 4.49175, Token Loss: 1.92660\n",
      "Step [1400/1000000], Loss: 5.69713, Vocab Loss: 3.74921, Token Loss: 1.90313\n",
      "Step [1410/1000000], Loss: 5.31018, Vocab Loss: 3.08598, Token Loss: 1.64699\n",
      "Step [1420/1000000], Loss: 5.52004, Vocab Loss: 2.64124, Token Loss: 1.53895\n",
      "Step [1430/1000000], Loss: 5.49408, Vocab Loss: 3.30935, Token Loss: 1.63470\n",
      "Step [1440/1000000], Loss: 5.24000, Vocab Loss: 3.10511, Token Loss: 1.83634\n",
      "Step [1450/1000000], Loss: 5.48118, Vocab Loss: 4.61812, Token Loss: 1.49709\n",
      "Step [1460/1000000], Loss: 5.48298, Vocab Loss: 3.05566, Token Loss: 1.71858\n",
      "Step [1470/1000000], Loss: 5.43021, Vocab Loss: 4.47450, Token Loss: 1.73708\n",
      "Step [1480/1000000], Loss: 5.37098, Vocab Loss: 4.44025, Token Loss: 1.88795\n",
      "Step [1490/1000000], Loss: 5.16594, Vocab Loss: 4.21855, Token Loss: 1.85677\n",
      "Step [1500/1000000], Loss: 5.15425, Vocab Loss: 3.32389, Token Loss: 1.73156\n",
      "Step [1510/1000000], Loss: 5.33574, Vocab Loss: 3.85550, Token Loss: 1.56332\n",
      "Step [1520/1000000], Loss: 5.11023, Vocab Loss: 4.54421, Token Loss: 1.84535\n",
      "Step [1530/1000000], Loss: 5.57024, Vocab Loss: 3.39777, Token Loss: 1.76372\n",
      "Step [1540/1000000], Loss: 5.30764, Vocab Loss: 3.25398, Token Loss: 1.88135\n",
      "Step [1550/1000000], Loss: 5.43947, Vocab Loss: 2.30299, Token Loss: 1.41883\n",
      "Step [1560/1000000], Loss: 5.45582, Vocab Loss: 3.69391, Token Loss: 1.74754\n",
      "Step [1570/1000000], Loss: 5.29688, Vocab Loss: 3.72418, Token Loss: 1.56796\n",
      "Step [1580/1000000], Loss: 5.42498, Vocab Loss: 3.11620, Token Loss: 1.89991\n",
      "Step [1590/1000000], Loss: 5.45903, Vocab Loss: 3.67642, Token Loss: 1.62366\n",
      "Step [1600/1000000], Loss: 5.27905, Vocab Loss: 3.91112, Token Loss: 1.49918\n",
      "Step [1610/1000000], Loss: 4.89416, Vocab Loss: 2.02774, Token Loss: 2.05538\n",
      "Step [1620/1000000], Loss: 4.99817, Vocab Loss: 3.28824, Token Loss: 1.87415\n",
      "Step [1630/1000000], Loss: 5.00386, Vocab Loss: 2.96958, Token Loss: 1.76918\n",
      "Step [1640/1000000], Loss: 5.37644, Vocab Loss: 3.76097, Token Loss: 1.79856\n",
      "Step [1650/1000000], Loss: 5.28172, Vocab Loss: 2.54126, Token Loss: 1.38156\n",
      "Step [1660/1000000], Loss: 5.32398, Vocab Loss: 4.91995, Token Loss: 1.89560\n",
      "Step [1670/1000000], Loss: 5.07281, Vocab Loss: 4.26479, Token Loss: 1.77977\n",
      "Step [1680/1000000], Loss: 5.28226, Vocab Loss: 3.79362, Token Loss: 1.71281\n",
      "Step [1690/1000000], Loss: 5.45122, Vocab Loss: 3.85468, Token Loss: 1.76669\n",
      "Step [1700/1000000], Loss: 5.15535, Vocab Loss: 2.35102, Token Loss: 1.33442\n",
      "Step [1710/1000000], Loss: 5.80945, Vocab Loss: 4.10801, Token Loss: 1.97433\n",
      "Step [1720/1000000], Loss: 5.42065, Vocab Loss: 2.99248, Token Loss: 1.81584\n",
      "Step [1730/1000000], Loss: 5.20885, Vocab Loss: 2.99406, Token Loss: 1.58892\n",
      "Step [1740/1000000], Loss: 5.00811, Vocab Loss: 2.37010, Token Loss: 1.78358\n",
      "Step [1750/1000000], Loss: 5.29627, Vocab Loss: 3.77350, Token Loss: 1.55661\n",
      "Step [1760/1000000], Loss: 5.16264, Vocab Loss: 2.03584, Token Loss: 1.81961\n",
      "Step [1770/1000000], Loss: 5.23853, Vocab Loss: 3.32049, Token Loss: 1.81345\n",
      "Step [1780/1000000], Loss: 5.17357, Vocab Loss: 3.28144, Token Loss: 1.73398\n",
      "Step [1790/1000000], Loss: 5.04063, Vocab Loss: 3.99655, Token Loss: 1.98663\n",
      "Step [1800/1000000], Loss: 5.16523, Vocab Loss: 3.49471, Token Loss: 1.90094\n",
      "Step [1810/1000000], Loss: 5.19362, Vocab Loss: 3.30738, Token Loss: 1.69915\n",
      "Step [1820/1000000], Loss: 5.39113, Vocab Loss: 4.29805, Token Loss: 1.84178\n",
      "Step [1830/1000000], Loss: 5.80569, Vocab Loss: 3.27930, Token Loss: 1.31119\n",
      "Step [1840/1000000], Loss: 4.97854, Vocab Loss: 4.74767, Token Loss: 1.89829\n",
      "Step [1850/1000000], Loss: 5.43932, Vocab Loss: 2.41863, Token Loss: 1.21278\n",
      "Step [1860/1000000], Loss: 5.25355, Vocab Loss: 5.05133, Token Loss: 1.95076\n",
      "Step [1870/1000000], Loss: 5.32938, Vocab Loss: 3.61195, Token Loss: 1.20996\n",
      "Step [1880/1000000], Loss: 5.18741, Vocab Loss: 3.99172, Token Loss: 2.08639\n",
      "Step [1890/1000000], Loss: 4.91336, Vocab Loss: 2.52279, Token Loss: 1.69675\n",
      "Step [1900/1000000], Loss: 5.27855, Vocab Loss: 3.02541, Token Loss: 1.87222\n",
      "Step [1910/1000000], Loss: 5.24565, Vocab Loss: 3.50734, Token Loss: 1.35338\n",
      "Step [1920/1000000], Loss: 5.24233, Vocab Loss: 3.94841, Token Loss: 1.54602\n",
      "Step [1930/1000000], Loss: 5.33570, Vocab Loss: 3.81978, Token Loss: 1.99014\n",
      "Step [1940/1000000], Loss: 5.58438, Vocab Loss: 3.25454, Token Loss: 1.98089\n",
      "Step [1950/1000000], Loss: 5.35953, Vocab Loss: 2.68548, Token Loss: 1.41979\n",
      "Step [1960/1000000], Loss: 5.01090, Vocab Loss: 3.01442, Token Loss: 1.82053\n",
      "Step [1970/1000000], Loss: 5.38074, Vocab Loss: 3.89472, Token Loss: 2.04861\n",
      "Step [1980/1000000], Loss: 5.42593, Vocab Loss: 2.77769, Token Loss: 1.68617\n",
      "Step [1990/1000000], Loss: 5.19587, Vocab Loss: 2.97695, Token Loss: 1.73020\n",
      "Step [2000/1000000], Loss: 4.99102, Vocab Loss: 2.98408, Token Loss: 1.80089\n",
      "Step [2010/1000000], Loss: 5.14728, Vocab Loss: 3.58301, Token Loss: 1.79103\n",
      "Step [2020/1000000], Loss: 4.84702, Vocab Loss: 4.42988, Token Loss: 1.78009\n",
      "Step [2030/1000000], Loss: 5.17581, Vocab Loss: 3.48851, Token Loss: 2.03992\n",
      "Step [2040/1000000], Loss: 4.92125, Vocab Loss: 2.52907, Token Loss: 1.73861\n",
      "Step [2050/1000000], Loss: 5.03315, Vocab Loss: 2.64778, Token Loss: 1.24223\n",
      "Step [2060/1000000], Loss: 5.05619, Vocab Loss: 2.22161, Token Loss: 1.79875\n",
      "Step [2070/1000000], Loss: 4.99672, Vocab Loss: 3.91965, Token Loss: 1.80251\n",
      "Step [2080/1000000], Loss: 4.93083, Vocab Loss: 3.23676, Token Loss: 1.63039\n",
      "Step [2090/1000000], Loss: 4.83729, Vocab Loss: 3.82527, Token Loss: 1.90781\n",
      "Step [2100/1000000], Loss: 5.26786, Vocab Loss: 4.31987, Token Loss: 1.82052\n",
      "Step [2110/1000000], Loss: 5.21704, Vocab Loss: 4.56448, Token Loss: 2.04867\n",
      "Step [2120/1000000], Loss: 5.21885, Vocab Loss: 2.38423, Token Loss: 1.64839\n",
      "Step [2130/1000000], Loss: 4.91457, Vocab Loss: 2.03297, Token Loss: 1.78372\n",
      "Step [2140/1000000], Loss: 4.79168, Vocab Loss: 3.03694, Token Loss: 1.60683\n",
      "Step [2150/1000000], Loss: 5.41579, Vocab Loss: 2.83402, Token Loss: 1.46372\n",
      "Step [2160/1000000], Loss: 5.06483, Vocab Loss: 3.25118, Token Loss: 1.84696\n",
      "Step [2170/1000000], Loss: 5.30237, Vocab Loss: 2.49109, Token Loss: 1.72327\n",
      "Step [2180/1000000], Loss: 4.51074, Vocab Loss: 3.29346, Token Loss: 1.61249\n",
      "Step [2190/1000000], Loss: 5.27459, Vocab Loss: 3.00506, Token Loss: 1.70286\n",
      "Step [2200/1000000], Loss: 4.63783, Vocab Loss: 2.70181, Token Loss: 1.68828\n",
      "Step [2210/1000000], Loss: 4.98878, Vocab Loss: 3.80607, Token Loss: 1.87516\n",
      "Step [2220/1000000], Loss: 4.85726, Vocab Loss: 2.15048, Token Loss: 1.57268\n",
      "Step [2230/1000000], Loss: 5.29696, Vocab Loss: 3.34525, Token Loss: 1.84872\n",
      "Step [2240/1000000], Loss: 4.91137, Vocab Loss: 2.89976, Token Loss: 1.77050\n",
      "Step [2250/1000000], Loss: 4.91301, Vocab Loss: 2.83772, Token Loss: 1.78041\n",
      "Step [2260/1000000], Loss: 4.90702, Vocab Loss: 2.00396, Token Loss: 1.76589\n",
      "Step [2270/1000000], Loss: 5.12512, Vocab Loss: 2.60513, Token Loss: 1.38535\n",
      "Step [2280/1000000], Loss: 5.10502, Vocab Loss: 3.14720, Token Loss: 1.68970\n",
      "Step [2290/1000000], Loss: 4.55245, Vocab Loss: 3.73978, Token Loss: 1.50656\n",
      "Step [2300/1000000], Loss: 4.87625, Vocab Loss: 3.35744, Token Loss: 1.66083\n",
      "Step [2310/1000000], Loss: 5.49822, Vocab Loss: 3.83466, Token Loss: 1.85521\n",
      "Step [2320/1000000], Loss: 5.01539, Vocab Loss: 2.64491, Token Loss: 1.91771\n",
      "Step [2330/1000000], Loss: 4.81810, Vocab Loss: 2.71549, Token Loss: 1.67637\n",
      "Step [2340/1000000], Loss: 4.61612, Vocab Loss: 2.28120, Token Loss: 1.57092\n",
      "Step [2350/1000000], Loss: 4.83493, Vocab Loss: 3.59005, Token Loss: 1.65110\n",
      "Step [2360/1000000], Loss: 4.88770, Vocab Loss: 4.43644, Token Loss: 1.80236\n",
      "Step [2370/1000000], Loss: 4.90779, Vocab Loss: 3.17946, Token Loss: 1.92554\n",
      "Step [2380/1000000], Loss: 4.75486, Vocab Loss: 2.87299, Token Loss: 1.56825\n",
      "Step [2390/1000000], Loss: 4.88157, Vocab Loss: 4.14965, Token Loss: 1.89245\n",
      "Step [2400/1000000], Loss: 4.75322, Vocab Loss: 2.07215, Token Loss: 1.42971\n",
      "Step [2410/1000000], Loss: 4.81446, Vocab Loss: 4.21962, Token Loss: 1.58554\n",
      "Step [2420/1000000], Loss: 4.87947, Vocab Loss: 4.03483, Token Loss: 1.63783\n",
      "Step [2430/1000000], Loss: 4.57553, Vocab Loss: 2.73657, Token Loss: 1.66281\n",
      "Step [2440/1000000], Loss: 5.55538, Vocab Loss: 3.04404, Token Loss: 1.76683\n",
      "Step [2450/1000000], Loss: 4.84756, Vocab Loss: 3.50007, Token Loss: 2.01551\n",
      "Step [2460/1000000], Loss: 4.80874, Vocab Loss: 2.78057, Token Loss: 1.55428\n",
      "Step [2470/1000000], Loss: 4.99942, Vocab Loss: 2.53853, Token Loss: 1.73965\n",
      "Step [2480/1000000], Loss: 5.16855, Vocab Loss: 2.93163, Token Loss: 1.87252\n",
      "Step [2490/1000000], Loss: 4.77801, Vocab Loss: 3.91613, Token Loss: 1.97118\n",
      "Step [2500/1000000], Loss: 4.87114, Vocab Loss: 4.07451, Token Loss: 2.04684\n",
      "Step [2510/1000000], Loss: 5.13542, Vocab Loss: 3.71419, Token Loss: 1.76391\n",
      "Step [2520/1000000], Loss: 5.02670, Vocab Loss: 1.92695, Token Loss: 1.70811\n",
      "Step [2530/1000000], Loss: 4.95246, Vocab Loss: 3.34561, Token Loss: 1.59204\n",
      "Step [2540/1000000], Loss: 5.16029, Vocab Loss: 3.21729, Token Loss: 2.02265\n",
      "Step [2550/1000000], Loss: 5.19956, Vocab Loss: 4.03018, Token Loss: 1.69954\n",
      "Step [2560/1000000], Loss: 5.00537, Vocab Loss: 2.83605, Token Loss: 1.95758\n",
      "Step [2570/1000000], Loss: 4.71947, Vocab Loss: 3.87650, Token Loss: 1.89235\n",
      "Step [2580/1000000], Loss: 5.07227, Vocab Loss: 4.22582, Token Loss: 2.14348\n",
      "Step [2590/1000000], Loss: 5.13433, Vocab Loss: 4.22026, Token Loss: 1.88760\n",
      "Step [2600/1000000], Loss: 5.21683, Vocab Loss: 4.00825, Token Loss: 2.03288\n",
      "Step [2610/1000000], Loss: 4.73472, Vocab Loss: 2.74698, Token Loss: 1.53114\n",
      "Step [2620/1000000], Loss: 5.15089, Vocab Loss: 2.90642, Token Loss: 1.82563\n",
      "Step [2630/1000000], Loss: 4.71808, Vocab Loss: 3.85869, Token Loss: 1.92566\n",
      "Step [2640/1000000], Loss: 5.07466, Vocab Loss: 2.91016, Token Loss: 1.73917\n",
      "Step [2650/1000000], Loss: 4.84306, Vocab Loss: 2.94125, Token Loss: 1.81150\n",
      "Step [2660/1000000], Loss: 4.82960, Vocab Loss: 3.10242, Token Loss: 1.67375\n",
      "Step [2670/1000000], Loss: 5.11591, Vocab Loss: 2.90288, Token Loss: 1.62656\n",
      "Step [2680/1000000], Loss: 4.68409, Vocab Loss: 3.60923, Token Loss: 1.77076\n",
      "Step [2690/1000000], Loss: 5.36492, Vocab Loss: 2.94861, Token Loss: 1.68998\n",
      "Step [2700/1000000], Loss: 5.14889, Vocab Loss: 5.13899, Token Loss: 1.87331\n",
      "Step [2710/1000000], Loss: 4.80033, Vocab Loss: 2.49635, Token Loss: 1.90850\n",
      "Step [2720/1000000], Loss: 4.71016, Vocab Loss: 3.11104, Token Loss: 1.62430\n",
      "Step [2730/1000000], Loss: 4.24990, Vocab Loss: 4.07540, Token Loss: 1.88672\n",
      "Step [2740/1000000], Loss: 4.57942, Vocab Loss: 2.38570, Token Loss: 1.36436\n",
      "Step [2750/1000000], Loss: 4.56244, Vocab Loss: 3.18676, Token Loss: 1.78754\n",
      "Step [2760/1000000], Loss: 5.02468, Vocab Loss: 4.71862, Token Loss: 1.88394\n",
      "Step [2770/1000000], Loss: 4.45688, Vocab Loss: 2.90315, Token Loss: 1.62868\n",
      "Step [2780/1000000], Loss: 4.74360, Vocab Loss: 3.07936, Token Loss: 1.85746\n",
      "Step [2790/1000000], Loss: 4.54792, Vocab Loss: 3.18043, Token Loss: 1.67322\n",
      "Step [2800/1000000], Loss: 5.14343, Vocab Loss: 3.41549, Token Loss: 1.78377\n",
      "Step [2810/1000000], Loss: 4.93734, Vocab Loss: 3.69803, Token Loss: 1.48170\n",
      "Step [2820/1000000], Loss: 4.56164, Vocab Loss: 2.08626, Token Loss: 1.71611\n",
      "Step [2830/1000000], Loss: 4.32935, Vocab Loss: 2.94015, Token Loss: 1.67615\n",
      "Step [2840/1000000], Loss: 4.84095, Vocab Loss: 3.50883, Token Loss: 1.89975\n",
      "Step [2850/1000000], Loss: 5.10792, Vocab Loss: 3.61092, Token Loss: 1.79856\n",
      "Step [2860/1000000], Loss: 4.50832, Vocab Loss: 2.51637, Token Loss: 1.69381\n",
      "Step [2870/1000000], Loss: 4.88459, Vocab Loss: 3.87542, Token Loss: 1.81354\n",
      "Step [2880/1000000], Loss: 4.65540, Vocab Loss: 4.14666, Token Loss: 1.74845\n",
      "Step [2890/1000000], Loss: 4.71497, Vocab Loss: 3.15670, Token Loss: 1.54140\n",
      "Step [2900/1000000], Loss: 4.49389, Vocab Loss: 1.55557, Token Loss: 1.41602\n",
      "Step [2910/1000000], Loss: 4.57077, Vocab Loss: 3.75824, Token Loss: 1.50731\n",
      "Step [2920/1000000], Loss: 4.79737, Vocab Loss: 3.91297, Token Loss: 1.64804\n",
      "Step [2930/1000000], Loss: 4.48307, Vocab Loss: 1.91469, Token Loss: 1.69395\n",
      "Step [2940/1000000], Loss: 4.97156, Vocab Loss: 2.77734, Token Loss: 1.70365\n",
      "Step [2950/1000000], Loss: 4.60331, Vocab Loss: 2.88063, Token Loss: 1.70282\n",
      "Step [2960/1000000], Loss: 4.37019, Vocab Loss: 2.41748, Token Loss: 1.47625\n",
      "Step [2970/1000000], Loss: 4.62816, Vocab Loss: 3.28614, Token Loss: 1.82004\n",
      "Step [2980/1000000], Loss: 3.75563, Vocab Loss: 2.17716, Token Loss: 1.68246\n",
      "Step [2990/1000000], Loss: 4.78908, Vocab Loss: 3.73506, Token Loss: 1.47065\n",
      "Step [3000/1000000], Loss: 4.69555, Vocab Loss: 2.47915, Token Loss: 1.66774\n",
      "Step [3010/1000000], Loss: 4.86149, Vocab Loss: 4.13764, Token Loss: 1.94246\n",
      "Step [3020/1000000], Loss: 5.32116, Vocab Loss: 5.40113, Token Loss: 1.90121\n",
      "Step [3030/1000000], Loss: 4.31751, Vocab Loss: 3.05651, Token Loss: 1.74945\n",
      "Step [3040/1000000], Loss: 4.71074, Vocab Loss: 3.39295, Token Loss: 1.65031\n",
      "Step [3050/1000000], Loss: 4.71910, Vocab Loss: 3.75772, Token Loss: 1.55833\n",
      "Step [3060/1000000], Loss: 4.26844, Vocab Loss: 1.98193, Token Loss: 1.00801\n",
      "Step [3070/1000000], Loss: 4.55462, Vocab Loss: 3.45631, Token Loss: 1.91385\n",
      "Step [3080/1000000], Loss: 5.10976, Vocab Loss: 2.72986, Token Loss: 1.79617\n",
      "Step [3090/1000000], Loss: 4.98475, Vocab Loss: 3.73635, Token Loss: 1.62429\n",
      "Step [3100/1000000], Loss: 4.90281, Vocab Loss: 3.54349, Token Loss: 1.71166\n",
      "Step [3110/1000000], Loss: 4.62954, Vocab Loss: 2.08887, Token Loss: 1.70897\n",
      "Step [3120/1000000], Loss: 4.64098, Vocab Loss: 3.41078, Token Loss: 1.24963\n",
      "Step [3130/1000000], Loss: 4.00826, Vocab Loss: 3.40445, Token Loss: 1.33685\n",
      "Step [3140/1000000], Loss: 4.81707, Vocab Loss: 3.11484, Token Loss: 1.42311\n",
      "Step [3150/1000000], Loss: 4.92065, Vocab Loss: 2.26382, Token Loss: 1.70609\n",
      "Step [3160/1000000], Loss: 4.32919, Vocab Loss: 1.68435, Token Loss: 1.75763\n",
      "Step [3170/1000000], Loss: 4.73742, Vocab Loss: 3.75577, Token Loss: 1.66695\n",
      "Step [3180/1000000], Loss: 4.83799, Vocab Loss: 3.21096, Token Loss: 1.62345\n",
      "Step [3190/1000000], Loss: 4.81150, Vocab Loss: 1.62215, Token Loss: 1.36700\n",
      "Step [3200/1000000], Loss: 5.02449, Vocab Loss: 1.88455, Token Loss: 1.54368\n",
      "Step [3210/1000000], Loss: 5.07317, Vocab Loss: 3.99618, Token Loss: 1.80325\n",
      "Step [3220/1000000], Loss: 5.18783, Vocab Loss: 4.83993, Token Loss: 1.68689\n",
      "Step [3230/1000000], Loss: 4.56806, Vocab Loss: 2.39659, Token Loss: 1.21136\n",
      "Step [3240/1000000], Loss: 4.52603, Vocab Loss: 2.25527, Token Loss: 1.45251\n",
      "Step [3250/1000000], Loss: 4.60542, Vocab Loss: 4.04424, Token Loss: 1.94650\n",
      "Step [3260/1000000], Loss: 3.89375, Vocab Loss: 3.71619, Token Loss: 1.48953\n",
      "Step [3270/1000000], Loss: 4.74737, Vocab Loss: 2.29213, Token Loss: 1.62434\n",
      "Step [3280/1000000], Loss: 4.23972, Vocab Loss: 2.73117, Token Loss: 1.81126\n",
      "Step [3290/1000000], Loss: 4.12512, Vocab Loss: 4.53390, Token Loss: 1.84771\n",
      "Step [3300/1000000], Loss: 4.53830, Vocab Loss: 3.79610, Token Loss: 1.68830\n",
      "Step [3310/1000000], Loss: 4.45787, Vocab Loss: 3.55265, Token Loss: 1.74624\n",
      "Step [3320/1000000], Loss: 4.55505, Vocab Loss: 3.52734, Token Loss: 1.65423\n",
      "Step [3330/1000000], Loss: 4.52901, Vocab Loss: 1.60909, Token Loss: 1.36549\n",
      "Step [3340/1000000], Loss: 4.68261, Vocab Loss: 2.36739, Token Loss: 1.54156\n",
      "Step [3350/1000000], Loss: 4.21345, Vocab Loss: 1.85258, Token Loss: 1.38000\n",
      "Step [3360/1000000], Loss: 4.04653, Vocab Loss: 2.87264, Token Loss: 1.48301\n",
      "Step [3370/1000000], Loss: 4.03468, Vocab Loss: 3.38318, Token Loss: 1.72902\n",
      "Step [3380/1000000], Loss: 4.95082, Vocab Loss: 1.61565, Token Loss: 1.55545\n",
      "Step [3390/1000000], Loss: 4.25178, Vocab Loss: 2.38873, Token Loss: 1.41833\n",
      "Step [3400/1000000], Loss: 5.08833, Vocab Loss: 4.33840, Token Loss: 1.76661\n",
      "Step [3410/1000000], Loss: 4.25995, Vocab Loss: 3.20563, Token Loss: 1.43094\n",
      "Step [3420/1000000], Loss: 3.87509, Vocab Loss: 2.25395, Token Loss: 1.30134\n",
      "Step [3430/1000000], Loss: 4.86181, Vocab Loss: 4.20589, Token Loss: 1.94598\n",
      "Step [3440/1000000], Loss: 4.51712, Vocab Loss: 3.25423, Token Loss: 1.82294\n",
      "Step [3450/1000000], Loss: 4.41404, Vocab Loss: 2.22939, Token Loss: 1.74509\n",
      "Step [3460/1000000], Loss: 4.87151, Vocab Loss: 3.53609, Token Loss: 1.67439\n",
      "Step [3470/1000000], Loss: 4.62543, Vocab Loss: 3.69491, Token Loss: 1.76843\n",
      "Step [3480/1000000], Loss: 4.36624, Vocab Loss: 3.31197, Token Loss: 1.88007\n",
      "Step [3490/1000000], Loss: 4.53982, Vocab Loss: 3.56558, Token Loss: 1.71718\n",
      "Step [3500/1000000], Loss: 4.31993, Vocab Loss: 3.14040, Token Loss: 1.77339\n",
      "Step [3510/1000000], Loss: 4.25277, Vocab Loss: 1.77389, Token Loss: 1.03776\n",
      "Step [3520/1000000], Loss: 4.74787, Vocab Loss: 2.23154, Token Loss: 1.47356\n",
      "Step [3530/1000000], Loss: 4.71058, Vocab Loss: 2.25323, Token Loss: 1.58192\n",
      "Step [3540/1000000], Loss: 3.75558, Vocab Loss: 2.69209, Token Loss: 1.33084\n",
      "Step [3550/1000000], Loss: 3.97622, Vocab Loss: 3.96628, Token Loss: 1.52735\n",
      "Step [3560/1000000], Loss: 5.13460, Vocab Loss: 4.68732, Token Loss: 2.08387\n",
      "Step [3570/1000000], Loss: 4.49571, Vocab Loss: 2.27133, Token Loss: 1.23188\n",
      "Step [3580/1000000], Loss: 5.10483, Vocab Loss: 4.01485, Token Loss: 1.70941\n",
      "Step [3590/1000000], Loss: 4.71934, Vocab Loss: 1.86483, Token Loss: 1.36611\n",
      "Step [3600/1000000], Loss: 4.79467, Vocab Loss: 2.47566, Token Loss: 1.41314\n",
      "Step [3610/1000000], Loss: 4.81063, Vocab Loss: 3.91672, Token Loss: 1.52339\n",
      "Step [3620/1000000], Loss: 4.64728, Vocab Loss: 2.72184, Token Loss: 1.57221\n",
      "Step [3630/1000000], Loss: 4.22701, Vocab Loss: 2.50950, Token Loss: 1.81363\n",
      "Step [3640/1000000], Loss: 4.44050, Vocab Loss: 3.70280, Token Loss: 1.73688\n",
      "Step [3650/1000000], Loss: 4.20833, Vocab Loss: 3.72229, Token Loss: 1.56358\n",
      "Step [3660/1000000], Loss: 4.34995, Vocab Loss: 2.06061, Token Loss: 1.52418\n",
      "Step [3670/1000000], Loss: 4.73935, Vocab Loss: 3.01156, Token Loss: 1.64015\n",
      "Step [3680/1000000], Loss: 4.48158, Vocab Loss: 3.17229, Token Loss: 1.53727\n",
      "Step [3690/1000000], Loss: 4.55315, Vocab Loss: 1.91093, Token Loss: 1.58280\n",
      "Step [3700/1000000], Loss: 4.42688, Vocab Loss: 3.02619, Token Loss: 1.57582\n",
      "Step [3710/1000000], Loss: 4.29327, Vocab Loss: 2.06626, Token Loss: 1.70366\n",
      "Step [3720/1000000], Loss: 4.74142, Vocab Loss: 2.12074, Token Loss: 1.21783\n",
      "Step [3730/1000000], Loss: 4.38512, Vocab Loss: 2.45545, Token Loss: 1.87161\n",
      "Step [3740/1000000], Loss: 4.73595, Vocab Loss: 3.23745, Token Loss: 1.88014\n",
      "Step [3750/1000000], Loss: 4.59769, Vocab Loss: 3.41292, Token Loss: 1.32565\n",
      "Step [3760/1000000], Loss: 4.12251, Vocab Loss: 1.31843, Token Loss: 1.21688\n",
      "Step [3770/1000000], Loss: 4.38977, Vocab Loss: 3.01813, Token Loss: 1.58714\n",
      "Step [3780/1000000], Loss: 4.02674, Vocab Loss: 1.72717, Token Loss: 1.46903\n",
      "Step [3790/1000000], Loss: 4.58650, Vocab Loss: 4.91722, Token Loss: 1.93227\n",
      "Step [3800/1000000], Loss: 4.82613, Vocab Loss: 3.56771, Token Loss: 1.74757\n",
      "Step [3810/1000000], Loss: 4.81707, Vocab Loss: 4.61815, Token Loss: 1.86577\n",
      "Step [3820/1000000], Loss: 4.15156, Vocab Loss: 3.33502, Token Loss: 1.68812\n",
      "Step [3830/1000000], Loss: 4.55874, Vocab Loss: 2.70426, Token Loss: 1.54735\n",
      "Step [3840/1000000], Loss: 4.51591, Vocab Loss: 2.89019, Token Loss: 1.68733\n",
      "Step [3850/1000000], Loss: 4.32686, Vocab Loss: 1.75224, Token Loss: 1.64288\n",
      "Step [3860/1000000], Loss: 4.66422, Vocab Loss: 3.16798, Token Loss: 1.60538\n",
      "Step [3870/1000000], Loss: 4.29641, Vocab Loss: 1.87731, Token Loss: 1.78615\n",
      "Step [3880/1000000], Loss: 4.80975, Vocab Loss: 2.44127, Token Loss: 1.73611\n",
      "Step [3890/1000000], Loss: 4.64100, Vocab Loss: 3.02511, Token Loss: 1.57330\n",
      "Step [3900/1000000], Loss: 4.65922, Vocab Loss: 2.45836, Token Loss: 1.09434\n",
      "Step [3910/1000000], Loss: 4.41092, Vocab Loss: 2.31824, Token Loss: 1.60694\n",
      "Step [3920/1000000], Loss: 3.54049, Vocab Loss: 1.96383, Token Loss: 1.64464\n",
      "Step [3930/1000000], Loss: 4.20283, Vocab Loss: 1.38356, Token Loss: 1.13122\n",
      "Step [3940/1000000], Loss: 4.09120, Vocab Loss: 1.87313, Token Loss: 1.22229\n",
      "Step [3950/1000000], Loss: 4.63652, Vocab Loss: 3.71204, Token Loss: 1.64162\n",
      "Step [3960/1000000], Loss: 4.64346, Vocab Loss: 3.96448, Token Loss: 1.77820\n",
      "Step [3970/1000000], Loss: 4.24522, Vocab Loss: 1.88289, Token Loss: 1.48308\n",
      "Step [3980/1000000], Loss: 4.43743, Vocab Loss: 4.12411, Token Loss: 1.63654\n",
      "Step [3990/1000000], Loss: 4.57692, Vocab Loss: 2.84300, Token Loss: 1.38898\n",
      "Step [4000/1000000], Loss: 4.05990, Vocab Loss: 3.92056, Token Loss: 1.73233\n",
      "Step [4010/1000000], Loss: 4.60936, Vocab Loss: 2.41163, Token Loss: 1.83124\n",
      "Step [4020/1000000], Loss: 4.08930, Vocab Loss: 2.62991, Token Loss: 1.68950\n",
      "Step [4030/1000000], Loss: 4.80538, Vocab Loss: 4.52379, Token Loss: 2.00549\n",
      "Step [4040/1000000], Loss: 4.23634, Vocab Loss: 1.47449, Token Loss: 1.41235\n",
      "Step [4050/1000000], Loss: 4.26012, Vocab Loss: 3.98669, Token Loss: 1.78518\n",
      "Step [4060/1000000], Loss: 3.79657, Vocab Loss: 2.02579, Token Loss: 1.58932\n",
      "Step [4070/1000000], Loss: 4.25717, Vocab Loss: 1.87124, Token Loss: 1.53787\n",
      "Step [4080/1000000], Loss: 4.36878, Vocab Loss: 4.40936, Token Loss: 1.49362\n",
      "Step [4090/1000000], Loss: 3.85928, Vocab Loss: 2.77452, Token Loss: 1.64409\n",
      "Step [4100/1000000], Loss: 4.24012, Vocab Loss: 2.27618, Token Loss: 1.18227\n",
      "Step [4110/1000000], Loss: 4.41177, Vocab Loss: 1.85282, Token Loss: 1.41912\n",
      "Step [4120/1000000], Loss: 4.22533, Vocab Loss: 2.01518, Token Loss: 1.47308\n",
      "Step [4130/1000000], Loss: 4.59372, Vocab Loss: 3.14657, Token Loss: 1.85047\n",
      "Step [4140/1000000], Loss: 4.66132, Vocab Loss: 3.34445, Token Loss: 1.92669\n",
      "Step [4150/1000000], Loss: 4.23735, Vocab Loss: 3.71771, Token Loss: 1.22960\n",
      "Step [4160/1000000], Loss: 3.68904, Vocab Loss: 2.13643, Token Loss: 1.44978\n",
      "Step [4170/1000000], Loss: 3.87696, Vocab Loss: 2.36668, Token Loss: 1.64193\n",
      "Step [4180/1000000], Loss: 4.63466, Vocab Loss: 4.56377, Token Loss: 2.08139\n",
      "Step [4190/1000000], Loss: 3.92294, Vocab Loss: 1.64894, Token Loss: 1.39764\n",
      "Step [4200/1000000], Loss: 3.82824, Vocab Loss: 3.21647, Token Loss: 1.71212\n",
      "Step [4210/1000000], Loss: 4.61101, Vocab Loss: 4.99775, Token Loss: 2.01205\n",
      "Step [4220/1000000], Loss: 3.83928, Vocab Loss: 1.34859, Token Loss: 1.07242\n",
      "Step [4230/1000000], Loss: 4.23727, Vocab Loss: 2.90517, Token Loss: 1.47021\n",
      "Step [4240/1000000], Loss: 4.63316, Vocab Loss: 3.45299, Token Loss: 1.61430\n",
      "Step [4250/1000000], Loss: 4.53594, Vocab Loss: 1.88700, Token Loss: 1.47868\n",
      "Step [4260/1000000], Loss: 4.51258, Vocab Loss: 3.63820, Token Loss: 1.48202\n",
      "Step [4270/1000000], Loss: 4.14770, Vocab Loss: 2.72987, Token Loss: 1.45363\n",
      "Step [4280/1000000], Loss: 4.22281, Vocab Loss: 2.12218, Token Loss: 1.20917\n",
      "Step [4290/1000000], Loss: 4.13856, Vocab Loss: 2.34157, Token Loss: 1.46928\n",
      "Step [4300/1000000], Loss: 4.33334, Vocab Loss: 3.36774, Token Loss: 1.41856\n",
      "Step [4310/1000000], Loss: 4.28272, Vocab Loss: 2.35276, Token Loss: 1.38461\n",
      "Step [4320/1000000], Loss: 3.63869, Vocab Loss: 2.47370, Token Loss: 1.08630\n",
      "Step [4330/1000000], Loss: 4.39678, Vocab Loss: 2.48687, Token Loss: 1.61000\n",
      "Step [4340/1000000], Loss: 4.70112, Vocab Loss: 3.00079, Token Loss: 1.55622\n",
      "Step [4350/1000000], Loss: 4.18254, Vocab Loss: 3.41727, Token Loss: 1.70013\n",
      "Step [4360/1000000], Loss: 4.57691, Vocab Loss: 4.05588, Token Loss: 1.65733\n",
      "Step [4370/1000000], Loss: 3.94490, Vocab Loss: 1.99702, Token Loss: 1.25156\n",
      "Step [4380/1000000], Loss: 4.15371, Vocab Loss: 2.29570, Token Loss: 1.41120\n",
      "Step [4390/1000000], Loss: 3.89677, Vocab Loss: 1.98852, Token Loss: 1.59188\n",
      "Step [4400/1000000], Loss: 4.60691, Vocab Loss: 3.13830, Token Loss: 1.74864\n",
      "Step [4410/1000000], Loss: 4.29603, Vocab Loss: 4.60480, Token Loss: 1.79252\n",
      "Step [4420/1000000], Loss: 3.91829, Vocab Loss: 1.70310, Token Loss: 1.57519\n",
      "Step [4430/1000000], Loss: 3.87093, Vocab Loss: 1.30286, Token Loss: 1.30953\n",
      "Step [4440/1000000], Loss: 4.34711, Vocab Loss: 3.03036, Token Loss: 1.41446\n",
      "Step [4450/1000000], Loss: 4.56157, Vocab Loss: 3.55374, Token Loss: 1.64101\n",
      "Step [4460/1000000], Loss: 4.35719, Vocab Loss: 2.88215, Token Loss: 1.70056\n",
      "Step [4470/1000000], Loss: 4.52774, Vocab Loss: 3.54195, Token Loss: 1.86318\n",
      "Step [4480/1000000], Loss: 3.96705, Vocab Loss: 1.47136, Token Loss: 1.54153\n",
      "Step [4490/1000000], Loss: 4.29468, Vocab Loss: 1.65155, Token Loss: 1.65073\n",
      "Step [4500/1000000], Loss: 4.29743, Vocab Loss: 3.56157, Token Loss: 1.84551\n",
      "Step [4510/1000000], Loss: 4.41212, Vocab Loss: 1.71640, Token Loss: 1.28488\n",
      "Step [4520/1000000], Loss: 3.89416, Vocab Loss: 2.41111, Token Loss: 1.28783\n",
      "Step [4530/1000000], Loss: 4.66758, Vocab Loss: 3.27654, Token Loss: 1.57706\n",
      "Step [4540/1000000], Loss: 3.80811, Vocab Loss: 3.59072, Token Loss: 1.42187\n",
      "Step [4550/1000000], Loss: 4.24517, Vocab Loss: 1.75948, Token Loss: 1.23358\n",
      "Step [4560/1000000], Loss: 4.36889, Vocab Loss: 2.10682, Token Loss: 1.40062\n",
      "Step [4570/1000000], Loss: 4.64299, Vocab Loss: 1.53671, Token Loss: 0.98914\n",
      "Step [4580/1000000], Loss: 3.83737, Vocab Loss: 2.38449, Token Loss: 1.44028\n",
      "Step [4590/1000000], Loss: 3.95133, Vocab Loss: 3.41064, Token Loss: 1.68455\n",
      "Step [4600/1000000], Loss: 3.68767, Vocab Loss: 1.96046, Token Loss: 1.58930\n",
      "Step [4610/1000000], Loss: 4.32461, Vocab Loss: 3.01857, Token Loss: 1.58938\n",
      "Step [4620/1000000], Loss: 4.10737, Vocab Loss: 4.05303, Token Loss: 1.63057\n",
      "Step [4630/1000000], Loss: 3.84819, Vocab Loss: 2.11223, Token Loss: 1.39983\n",
      "Step [4640/1000000], Loss: 4.18481, Vocab Loss: 2.07392, Token Loss: 1.86590\n",
      "Step [4650/1000000], Loss: 4.28172, Vocab Loss: 1.82408, Token Loss: 1.92182\n",
      "Step [4660/1000000], Loss: 4.16976, Vocab Loss: 1.56810, Token Loss: 1.34786\n",
      "Step [4670/1000000], Loss: 3.65565, Vocab Loss: 1.54479, Token Loss: 1.04488\n",
      "Step [4680/1000000], Loss: 4.22714, Vocab Loss: 1.96550, Token Loss: 1.72226\n",
      "Step [4690/1000000], Loss: 3.58947, Vocab Loss: 3.08375, Token Loss: 1.49147\n",
      "Step [4700/1000000], Loss: 3.78953, Vocab Loss: 1.85469, Token Loss: 1.64798\n",
      "Step [4710/1000000], Loss: 3.97280, Vocab Loss: 2.03608, Token Loss: 1.47110\n",
      "Step [4720/1000000], Loss: 4.01485, Vocab Loss: 2.93434, Token Loss: 1.67049\n",
      "Step [4730/1000000], Loss: 3.72549, Vocab Loss: 1.47599, Token Loss: 1.41506\n",
      "Step [4740/1000000], Loss: 3.79533, Vocab Loss: 2.50274, Token Loss: 1.37429\n",
      "Step [4750/1000000], Loss: 3.64591, Vocab Loss: 2.64166, Token Loss: 1.60007\n",
      "Step [4760/1000000], Loss: 3.90842, Vocab Loss: 2.66499, Token Loss: 1.94083\n",
      "Step [4770/1000000], Loss: 4.66490, Vocab Loss: 3.49978, Token Loss: 1.74699\n",
      "Step [4780/1000000], Loss: 4.29273, Vocab Loss: 1.33390, Token Loss: 1.29361\n",
      "Step [4790/1000000], Loss: 3.84916, Vocab Loss: 2.88978, Token Loss: 1.76945\n",
      "Step [4800/1000000], Loss: 3.97271, Vocab Loss: 2.28172, Token Loss: 1.46258\n",
      "Step [4810/1000000], Loss: 4.43729, Vocab Loss: 3.75769, Token Loss: 1.85692\n",
      "Step [4820/1000000], Loss: 4.13869, Vocab Loss: 2.49276, Token Loss: 1.70531\n",
      "Step [4830/1000000], Loss: 4.26218, Vocab Loss: 2.69528, Token Loss: 1.66518\n",
      "Step [4840/1000000], Loss: 4.33982, Vocab Loss: 2.73397, Token Loss: 1.88556\n",
      "Step [4850/1000000], Loss: 3.76675, Vocab Loss: 1.26857, Token Loss: 1.44988\n",
      "Step [4860/1000000], Loss: 3.82527, Vocab Loss: 2.41740, Token Loss: 1.87983\n",
      "Step [4870/1000000], Loss: 3.99003, Vocab Loss: 2.55257, Token Loss: 1.71959\n",
      "Step [4880/1000000], Loss: 4.00871, Vocab Loss: 1.68126, Token Loss: 1.72773\n",
      "Step [4890/1000000], Loss: 3.89540, Vocab Loss: 3.56944, Token Loss: 1.48429\n",
      "Step [4900/1000000], Loss: 4.49068, Vocab Loss: 3.05555, Token Loss: 1.35024\n",
      "Step [4910/1000000], Loss: 4.10930, Vocab Loss: 1.73952, Token Loss: 1.31772\n",
      "Step [4920/1000000], Loss: 4.12566, Vocab Loss: 1.61028, Token Loss: 1.32858\n",
      "Step [4930/1000000], Loss: 4.07713, Vocab Loss: 2.43660, Token Loss: 1.37745\n",
      "Step [4940/1000000], Loss: 3.89602, Vocab Loss: 1.93830, Token Loss: 1.66967\n",
      "Step [4950/1000000], Loss: 4.22988, Vocab Loss: 2.77579, Token Loss: 1.71059\n",
      "Step [4960/1000000], Loss: 3.84432, Vocab Loss: 3.90859, Token Loss: 1.54822\n",
      "Step [4970/1000000], Loss: 4.00769, Vocab Loss: 1.41605, Token Loss: 1.37626\n",
      "Step [4980/1000000], Loss: 3.95539, Vocab Loss: 2.46786, Token Loss: 1.63184\n",
      "Step [4990/1000000], Loss: 3.92558, Vocab Loss: 3.11096, Token Loss: 1.75082\n",
      "Step [5000/1000000], Loss: 3.84202, Vocab Loss: 2.99727, Token Loss: 1.55470\n",
      "Saving..\n",
      "Step [5010/1000000], Loss: 3.87037, Vocab Loss: 2.22928, Token Loss: 1.33976\n",
      "Step [5020/1000000], Loss: 3.65009, Vocab Loss: 1.89995, Token Loss: 1.15049\n",
      "Step [5030/1000000], Loss: 4.00925, Vocab Loss: 1.99455, Token Loss: 1.49059\n",
      "Step [5040/1000000], Loss: 3.78710, Vocab Loss: 3.11532, Token Loss: 1.63387\n",
      "Step [5050/1000000], Loss: 3.86227, Vocab Loss: 1.78498, Token Loss: 0.84665\n",
      "Step [5060/1000000], Loss: 4.11505, Vocab Loss: 2.69891, Token Loss: 1.79388\n",
      "Step [5070/1000000], Loss: 3.86308, Vocab Loss: 2.45935, Token Loss: 1.50997\n",
      "Step [5080/1000000], Loss: 3.64049, Vocab Loss: 2.56043, Token Loss: 1.53241\n",
      "Step [5090/1000000], Loss: 3.51339, Vocab Loss: 1.73778, Token Loss: 0.99146\n",
      "Step [5100/1000000], Loss: 3.92924, Vocab Loss: 2.68125, Token Loss: 1.53090\n",
      "Step [5110/1000000], Loss: 4.00913, Vocab Loss: 1.90270, Token Loss: 1.63527\n",
      "Step [5120/1000000], Loss: 4.27473, Vocab Loss: 2.88246, Token Loss: 1.78761\n",
      "Step [5130/1000000], Loss: 3.65592, Vocab Loss: 1.47770, Token Loss: 1.38804\n",
      "Step [5140/1000000], Loss: 4.24957, Vocab Loss: 2.91455, Token Loss: 1.82871\n",
      "Step [5150/1000000], Loss: 3.57178, Vocab Loss: 1.30236, Token Loss: 1.45658\n",
      "Step [5160/1000000], Loss: 3.81320, Vocab Loss: 1.24675, Token Loss: 1.44023\n",
      "Step [5170/1000000], Loss: 4.25411, Vocab Loss: 3.49948, Token Loss: 1.83696\n",
      "Step [5180/1000000], Loss: 4.00837, Vocab Loss: 3.76492, Token Loss: 1.78687\n",
      "Step [5190/1000000], Loss: 3.66761, Vocab Loss: 1.46781, Token Loss: 1.43428\n",
      "Step [5200/1000000], Loss: 4.03060, Vocab Loss: 1.22145, Token Loss: 1.45018\n",
      "Step [5210/1000000], Loss: 3.84839, Vocab Loss: 2.47432, Token Loss: 1.58485\n",
      "Step [5220/1000000], Loss: 3.52239, Vocab Loss: 1.61038, Token Loss: 1.15543\n",
      "Step [5230/1000000], Loss: 3.61530, Vocab Loss: 2.08619, Token Loss: 1.17949\n",
      "Step [5240/1000000], Loss: 3.90641, Vocab Loss: 2.04275, Token Loss: 1.41316\n",
      "Step [5250/1000000], Loss: 3.83913, Vocab Loss: 2.44574, Token Loss: 1.65483\n",
      "Step [5260/1000000], Loss: 3.57990, Vocab Loss: 1.26400, Token Loss: 1.06056\n",
      "Step [5270/1000000], Loss: 3.53247, Vocab Loss: 3.04769, Token Loss: 1.93104\n",
      "Step [5280/1000000], Loss: 4.02725, Vocab Loss: 2.89591, Token Loss: 1.73088\n",
      "Step [5290/1000000], Loss: 3.35794, Vocab Loss: 1.62285, Token Loss: 1.31270\n",
      "Step [5300/1000000], Loss: 3.63271, Vocab Loss: 2.71288, Token Loss: 1.62249\n",
      "Step [5310/1000000], Loss: 3.89050, Vocab Loss: 3.87154, Token Loss: 1.87311\n",
      "Step [5320/1000000], Loss: 3.73999, Vocab Loss: 1.40376, Token Loss: 1.33555\n",
      "Step [5330/1000000], Loss: 3.46189, Vocab Loss: 2.80434, Token Loss: 1.63954\n",
      "Step [5340/1000000], Loss: 3.60225, Vocab Loss: 1.96951, Token Loss: 1.91927\n",
      "Step [5350/1000000], Loss: 3.54445, Vocab Loss: 1.91362, Token Loss: 1.38022\n",
      "Step [5360/1000000], Loss: 3.54133, Vocab Loss: 1.96026, Token Loss: 1.62042\n",
      "Step [5370/1000000], Loss: 3.57677, Vocab Loss: 2.34599, Token Loss: 1.50339\n",
      "Step [5380/1000000], Loss: 4.04499, Vocab Loss: 2.91592, Token Loss: 1.60395\n",
      "Step [5390/1000000], Loss: 3.71782, Vocab Loss: 3.05574, Token Loss: 1.55260\n",
      "Step [5400/1000000], Loss: 3.83490, Vocab Loss: 2.17059, Token Loss: 1.39345\n",
      "Step [5410/1000000], Loss: 3.30231, Vocab Loss: 1.13708, Token Loss: 1.32227\n",
      "Step [5420/1000000], Loss: 3.86704, Vocab Loss: 3.33834, Token Loss: 1.84546\n",
      "Step [5430/1000000], Loss: 3.93030, Vocab Loss: 2.85062, Token Loss: 1.58647\n",
      "Step [5440/1000000], Loss: 3.55406, Vocab Loss: 3.27615, Token Loss: 1.88108\n",
      "Step [5450/1000000], Loss: 3.52264, Vocab Loss: 2.13118, Token Loss: 1.66385\n",
      "Step [5460/1000000], Loss: 3.72247, Vocab Loss: 3.93190, Token Loss: 1.67274\n",
      "Step [5470/1000000], Loss: 3.99983, Vocab Loss: 1.87015, Token Loss: 1.53085\n",
      "Step [5480/1000000], Loss: 3.33342, Vocab Loss: 1.50417, Token Loss: 1.01155\n",
      "Step [5490/1000000], Loss: 3.55326, Vocab Loss: 1.36665, Token Loss: 1.09827\n",
      "Step [5500/1000000], Loss: 4.24398, Vocab Loss: 2.66558, Token Loss: 1.31011\n",
      "Step [5510/1000000], Loss: 3.59772, Vocab Loss: 2.30150, Token Loss: 1.69753\n",
      "Step [5520/1000000], Loss: 3.72350, Vocab Loss: 2.01461, Token Loss: 1.66978\n",
      "Step [5530/1000000], Loss: 3.60075, Vocab Loss: 1.40418, Token Loss: 1.66614\n",
      "Step [5540/1000000], Loss: 3.61313, Vocab Loss: 2.18833, Token Loss: 1.56030\n",
      "Step [5550/1000000], Loss: 3.73789, Vocab Loss: 3.12092, Token Loss: 1.71971\n",
      "Step [5560/1000000], Loss: 4.00013, Vocab Loss: 2.24717, Token Loss: 1.59731\n",
      "Step [5570/1000000], Loss: 3.53813, Vocab Loss: 1.26321, Token Loss: 1.52433\n",
      "Step [5580/1000000], Loss: 3.81392, Vocab Loss: 2.15634, Token Loss: 1.33057\n",
      "Step [5590/1000000], Loss: 3.68683, Vocab Loss: 1.08241, Token Loss: 1.34526\n",
      "Step [5600/1000000], Loss: 3.34490, Vocab Loss: 1.76661, Token Loss: 1.40660\n",
      "Step [5610/1000000], Loss: 3.46848, Vocab Loss: 1.33219, Token Loss: 1.65619\n",
      "Step [5620/1000000], Loss: 3.33726, Vocab Loss: 1.80205, Token Loss: 0.99511\n",
      "Step [5630/1000000], Loss: 3.15893, Vocab Loss: 1.04633, Token Loss: 0.95285\n",
      "Step [5640/1000000], Loss: 3.42297, Vocab Loss: 1.58006, Token Loss: 1.38870\n",
      "Step [5650/1000000], Loss: 3.55063, Vocab Loss: 2.05566, Token Loss: 1.56664\n",
      "Step [5660/1000000], Loss: 3.71200, Vocab Loss: 1.43459, Token Loss: 1.82058\n",
      "Step [5670/1000000], Loss: 3.76724, Vocab Loss: 2.55729, Token Loss: 1.38364\n",
      "Step [5680/1000000], Loss: 3.52007, Vocab Loss: 1.10407, Token Loss: 1.18500\n",
      "Step [5690/1000000], Loss: 3.56658, Vocab Loss: 3.23276, Token Loss: 1.92066\n",
      "Step [5700/1000000], Loss: 3.63946, Vocab Loss: 2.43862, Token Loss: 1.51402\n",
      "Step [5710/1000000], Loss: 3.44723, Vocab Loss: 1.55740, Token Loss: 1.45052\n",
      "Step [5720/1000000], Loss: 3.42279, Vocab Loss: 2.21763, Token Loss: 1.78374\n",
      "Step [5730/1000000], Loss: 3.42039, Vocab Loss: 3.11290, Token Loss: 1.75075\n",
      "Step [5740/1000000], Loss: 3.36289, Vocab Loss: 1.91426, Token Loss: 1.82770\n",
      "Step [5750/1000000], Loss: 3.41335, Vocab Loss: 1.18844, Token Loss: 1.45628\n",
      "Step [5760/1000000], Loss: 4.13314, Vocab Loss: 2.53191, Token Loss: 1.76562\n",
      "Step [5770/1000000], Loss: 3.66790, Vocab Loss: 0.95232, Token Loss: 1.13980\n",
      "Step [5780/1000000], Loss: 3.18195, Vocab Loss: 2.31268, Token Loss: 1.25800\n",
      "Step [5790/1000000], Loss: 2.90821, Vocab Loss: 2.44331, Token Loss: 0.98545\n",
      "Step [5800/1000000], Loss: 3.34422, Vocab Loss: 1.67446, Token Loss: 1.58313\n",
      "Step [5810/1000000], Loss: 3.77577, Vocab Loss: 2.91592, Token Loss: 1.63243\n",
      "Step [5820/1000000], Loss: 3.28339, Vocab Loss: 2.25532, Token Loss: 1.69899\n",
      "Step [5830/1000000], Loss: 3.36165, Vocab Loss: 2.95692, Token Loss: 1.85449\n",
      "Step [5840/1000000], Loss: 3.22017, Vocab Loss: 1.42796, Token Loss: 1.11294\n",
      "Step [5850/1000000], Loss: 3.67310, Vocab Loss: 1.51648, Token Loss: 0.95306\n",
      "Step [5860/1000000], Loss: 3.39816, Vocab Loss: 2.12867, Token Loss: 1.43161\n",
      "Step [5870/1000000], Loss: 3.47448, Vocab Loss: 2.27969, Token Loss: 1.69706\n",
      "Step [5880/1000000], Loss: 3.72706, Vocab Loss: 2.27333, Token Loss: 1.70540\n",
      "Step [5890/1000000], Loss: 3.04369, Vocab Loss: 2.83207, Token Loss: 1.90227\n",
      "Step [5900/1000000], Loss: 3.68247, Vocab Loss: 1.37773, Token Loss: 1.29510\n",
      "Step [5910/1000000], Loss: 3.54440, Vocab Loss: 1.75784, Token Loss: 1.32383\n",
      "Step [5920/1000000], Loss: 3.73334, Vocab Loss: 1.68593, Token Loss: 1.40673\n",
      "Step [5930/1000000], Loss: 3.35174, Vocab Loss: 1.96205, Token Loss: 1.44121\n",
      "Step [5940/1000000], Loss: 3.83128, Vocab Loss: 2.26457, Token Loss: 1.42115\n",
      "Step [5950/1000000], Loss: 3.87212, Vocab Loss: 1.45696, Token Loss: 1.06231\n",
      "Step [5960/1000000], Loss: 3.54495, Vocab Loss: 1.82253, Token Loss: 1.49028\n",
      "Step [5970/1000000], Loss: 3.46142, Vocab Loss: 1.89763, Token Loss: 1.67705\n",
      "Step [5980/1000000], Loss: 3.04208, Vocab Loss: 0.92226, Token Loss: 1.17167\n",
      "Step [5990/1000000], Loss: 3.52921, Vocab Loss: 2.52650, Token Loss: 1.59602\n",
      "Step [6000/1000000], Loss: 3.46981, Vocab Loss: 1.12981, Token Loss: 1.64763\n",
      "Step [6010/1000000], Loss: 3.54688, Vocab Loss: 1.71723, Token Loss: 1.45548\n",
      "Step [6020/1000000], Loss: 3.35154, Vocab Loss: 1.29659, Token Loss: 1.27323\n",
      "Step [6030/1000000], Loss: 3.40864, Vocab Loss: 1.86574, Token Loss: 1.39074\n",
      "Step [6040/1000000], Loss: 2.96639, Vocab Loss: 1.45667, Token Loss: 1.67062\n",
      "Step [6050/1000000], Loss: 3.60402, Vocab Loss: 2.01348, Token Loss: 1.48996\n",
      "Step [6060/1000000], Loss: 2.81081, Vocab Loss: 1.37704, Token Loss: 1.48285\n",
      "Step [6070/1000000], Loss: 3.16822, Vocab Loss: 2.88494, Token Loss: 1.90002\n",
      "Step [6080/1000000], Loss: 3.19077, Vocab Loss: 1.80621, Token Loss: 1.83627\n",
      "Step [6090/1000000], Loss: 3.51865, Vocab Loss: 2.14377, Token Loss: 1.25152\n",
      "Step [6100/1000000], Loss: 3.38537, Vocab Loss: 2.51259, Token Loss: 1.73185\n",
      "Step [6110/1000000], Loss: 3.41209, Vocab Loss: 1.83279, Token Loss: 1.31516\n",
      "Step [6120/1000000], Loss: 3.26715, Vocab Loss: 2.32598, Token Loss: 1.71908\n",
      "Step [6130/1000000], Loss: 3.46057, Vocab Loss: 2.91260, Token Loss: 1.72802\n",
      "Step [6140/1000000], Loss: 3.59963, Vocab Loss: 2.06866, Token Loss: 1.60922\n",
      "Step [6150/1000000], Loss: 3.71154, Vocab Loss: 3.79194, Token Loss: 1.88136\n",
      "Step [6160/1000000], Loss: 3.58635, Vocab Loss: 2.02740, Token Loss: 1.25986\n",
      "Step [6170/1000000], Loss: 3.28593, Vocab Loss: 1.37574, Token Loss: 1.50800\n",
      "Step [6180/1000000], Loss: 3.37253, Vocab Loss: 1.70873, Token Loss: 1.27064\n",
      "Step [6190/1000000], Loss: 3.45015, Vocab Loss: 1.40347, Token Loss: 1.30127\n",
      "Step [6200/1000000], Loss: 3.71640, Vocab Loss: 3.81420, Token Loss: 2.05360\n",
      "Step [6210/1000000], Loss: 3.84468, Vocab Loss: 2.57685, Token Loss: 1.38544\n",
      "Step [6220/1000000], Loss: 3.42522, Vocab Loss: 0.99919, Token Loss: 1.32133\n",
      "Step [6230/1000000], Loss: 3.24909, Vocab Loss: 2.53252, Token Loss: 1.51397\n",
      "Step [6240/1000000], Loss: 3.34897, Vocab Loss: 2.17845, Token Loss: 1.48295\n",
      "Step [6250/1000000], Loss: 3.15455, Vocab Loss: 2.18007, Token Loss: 1.47310\n",
      "Step [6260/1000000], Loss: 3.33601, Vocab Loss: 1.76297, Token Loss: 1.68337\n",
      "Step [6270/1000000], Loss: 3.55199, Vocab Loss: 2.27965, Token Loss: 1.95392\n",
      "Step [6280/1000000], Loss: 3.38783, Vocab Loss: 0.96169, Token Loss: 0.98550\n",
      "Step [6290/1000000], Loss: 3.33490, Vocab Loss: 1.27013, Token Loss: 1.51007\n",
      "Step [6300/1000000], Loss: 3.38690, Vocab Loss: 0.94342, Token Loss: 1.42773\n",
      "Step [6310/1000000], Loss: 3.03219, Vocab Loss: 1.34541, Token Loss: 1.45597\n",
      "Step [6320/1000000], Loss: 3.28665, Vocab Loss: 1.58676, Token Loss: 1.37775\n",
      "Step [6330/1000000], Loss: 3.16311, Vocab Loss: 2.25682, Token Loss: 1.36770\n",
      "Step [6340/1000000], Loss: 3.11140, Vocab Loss: 2.39677, Token Loss: 1.80796\n",
      "Step [6350/1000000], Loss: 3.21607, Vocab Loss: 0.92025, Token Loss: 1.06345\n",
      "Step [6360/1000000], Loss: 3.70622, Vocab Loss: 1.70071, Token Loss: 1.35870\n",
      "Step [6370/1000000], Loss: 2.97537, Vocab Loss: 2.08650, Token Loss: 1.26854\n",
      "Step [6380/1000000], Loss: 3.48909, Vocab Loss: 2.49947, Token Loss: 1.37761\n",
      "Step [6390/1000000], Loss: 3.51810, Vocab Loss: 1.90265, Token Loss: 1.20857\n",
      "Step [6400/1000000], Loss: 2.90495, Vocab Loss: 1.54556, Token Loss: 1.37406\n",
      "Step [6410/1000000], Loss: 3.39013, Vocab Loss: 0.98283, Token Loss: 1.45246\n",
      "Step [6420/1000000], Loss: 3.38933, Vocab Loss: 1.79989, Token Loss: 1.38535\n",
      "Step [6430/1000000], Loss: 3.75207, Vocab Loss: 2.04931, Token Loss: 1.91397\n",
      "Step [6440/1000000], Loss: 3.28758, Vocab Loss: 2.08741, Token Loss: 1.37598\n",
      "Step [6450/1000000], Loss: 3.21158, Vocab Loss: 1.36360, Token Loss: 0.83846\n",
      "Step [6460/1000000], Loss: 3.27010, Vocab Loss: 3.25279, Token Loss: 1.55507\n",
      "Step [6470/1000000], Loss: 3.64831, Vocab Loss: 2.10288, Token Loss: 1.53333\n",
      "Step [6480/1000000], Loss: 3.33353, Vocab Loss: 1.89522, Token Loss: 1.70579\n",
      "Step [6490/1000000], Loss: 3.00106, Vocab Loss: 1.67860, Token Loss: 1.46925\n",
      "Step [6500/1000000], Loss: 3.47073, Vocab Loss: 2.03023, Token Loss: 1.27195\n",
      "Step [6510/1000000], Loss: 3.08567, Vocab Loss: 2.44323, Token Loss: 1.35098\n",
      "Step [6520/1000000], Loss: 3.13974, Vocab Loss: 0.94445, Token Loss: 1.16599\n",
      "Step [6530/1000000], Loss: 3.14043, Vocab Loss: 1.97056, Token Loss: 1.32841\n",
      "Step [6540/1000000], Loss: 3.62286, Vocab Loss: 1.84210, Token Loss: 1.51670\n",
      "Step [6550/1000000], Loss: 3.15029, Vocab Loss: 2.10045, Token Loss: 1.33885\n",
      "Step [6560/1000000], Loss: 3.51488, Vocab Loss: 2.00494, Token Loss: 1.51302\n",
      "Step [6570/1000000], Loss: 3.07818, Vocab Loss: 1.26629, Token Loss: 1.50098\n",
      "Step [6580/1000000], Loss: 3.13490, Vocab Loss: 1.07933, Token Loss: 0.96404\n",
      "Step [6590/1000000], Loss: 3.06071, Vocab Loss: 2.68029, Token Loss: 1.56813\n",
      "Step [6600/1000000], Loss: 3.31422, Vocab Loss: 2.95724, Token Loss: 1.91302\n",
      "Step [6610/1000000], Loss: 3.12172, Vocab Loss: 1.55073, Token Loss: 1.34656\n",
      "Step [6620/1000000], Loss: 3.10600, Vocab Loss: 2.18224, Token Loss: 1.70793\n",
      "Step [6630/1000000], Loss: 3.50457, Vocab Loss: 1.13285, Token Loss: 1.01518\n",
      "Step [6640/1000000], Loss: 2.98143, Vocab Loss: 2.33147, Token Loss: 1.17872\n",
      "Step [6650/1000000], Loss: 3.22411, Vocab Loss: 0.98398, Token Loss: 1.45782\n",
      "Step [6660/1000000], Loss: 3.42440, Vocab Loss: 1.16247, Token Loss: 0.97861\n",
      "Step [6670/1000000], Loss: 3.33169, Vocab Loss: 2.02084, Token Loss: 1.25054\n",
      "Step [6680/1000000], Loss: 3.31740, Vocab Loss: 1.95443, Token Loss: 1.74697\n",
      "Step [6690/1000000], Loss: 3.17242, Vocab Loss: 1.76712, Token Loss: 1.47744\n",
      "Step [6700/1000000], Loss: 3.41772, Vocab Loss: 1.35017, Token Loss: 1.30029\n",
      "Step [6710/1000000], Loss: 3.06914, Vocab Loss: 2.18135, Token Loss: 1.89496\n",
      "Step [6720/1000000], Loss: 3.21636, Vocab Loss: 2.41675, Token Loss: 1.57614\n",
      "Step [6730/1000000], Loss: 3.47224, Vocab Loss: 1.67338, Token Loss: 1.27388\n",
      "Step [6740/1000000], Loss: 3.05932, Vocab Loss: 2.00857, Token Loss: 1.69990\n",
      "Step [6750/1000000], Loss: 2.90843, Vocab Loss: 1.77702, Token Loss: 1.25861\n",
      "Step [6760/1000000], Loss: 3.17920, Vocab Loss: 1.74813, Token Loss: 1.59073\n",
      "Step [6770/1000000], Loss: 2.97225, Vocab Loss: 2.63707, Token Loss: 1.78702\n",
      "Step [6780/1000000], Loss: 3.01021, Vocab Loss: 1.90742, Token Loss: 1.30246\n",
      "Step [6790/1000000], Loss: 3.08987, Vocab Loss: 1.60580, Token Loss: 0.88852\n",
      "Step [6800/1000000], Loss: 3.13782, Vocab Loss: 0.84836, Token Loss: 1.37841\n",
      "Step [6810/1000000], Loss: 2.98555, Vocab Loss: 1.50420, Token Loss: 1.42950\n",
      "Step [6820/1000000], Loss: 3.13638, Vocab Loss: 2.61726, Token Loss: 1.63043\n",
      "Step [6830/1000000], Loss: 2.96760, Vocab Loss: 0.72497, Token Loss: 0.58571\n",
      "Step [6840/1000000], Loss: 3.20295, Vocab Loss: 2.42614, Token Loss: 1.48418\n",
      "Step [6850/1000000], Loss: 3.07659, Vocab Loss: 1.15235, Token Loss: 1.37080\n",
      "Step [6860/1000000], Loss: 3.18567, Vocab Loss: 1.40813, Token Loss: 1.43749\n",
      "Step [6870/1000000], Loss: 2.76019, Vocab Loss: 1.95177, Token Loss: 1.40862\n",
      "Step [6880/1000000], Loss: 3.15358, Vocab Loss: 1.31637, Token Loss: 1.55703\n",
      "Step [6890/1000000], Loss: 3.42159, Vocab Loss: 1.77649, Token Loss: 1.45630\n",
      "Step [6900/1000000], Loss: 2.95657, Vocab Loss: 1.02136, Token Loss: 1.14367\n",
      "Step [6910/1000000], Loss: 3.46649, Vocab Loss: 1.44969, Token Loss: 2.12898\n",
      "Step [6920/1000000], Loss: 3.20084, Vocab Loss: 2.23110, Token Loss: 1.28950\n",
      "Step [6930/1000000], Loss: 3.24565, Vocab Loss: 0.94024, Token Loss: 1.42786\n",
      "Step [6940/1000000], Loss: 3.02061, Vocab Loss: 0.96166, Token Loss: 0.85437\n",
      "Step [6950/1000000], Loss: 3.02599, Vocab Loss: 1.20917, Token Loss: 1.25648\n",
      "Step [6960/1000000], Loss: 2.90973, Vocab Loss: 1.30787, Token Loss: 0.87843\n",
      "Step [6970/1000000], Loss: 3.19954, Vocab Loss: 2.06619, Token Loss: 1.09786\n",
      "Step [6980/1000000], Loss: 3.25651, Vocab Loss: 1.28899, Token Loss: 1.05929\n",
      "Step [6990/1000000], Loss: 2.99674, Vocab Loss: 1.15899, Token Loss: 1.28912\n",
      "Step [7000/1000000], Loss: 3.05011, Vocab Loss: 1.07900, Token Loss: 1.11355\n",
      "Step [7010/1000000], Loss: 3.30092, Vocab Loss: 0.84182, Token Loss: 1.18412\n",
      "Step [7020/1000000], Loss: 3.34507, Vocab Loss: 1.64170, Token Loss: 1.49209\n",
      "Step [7030/1000000], Loss: 2.72119, Vocab Loss: 1.26378, Token Loss: 1.47533\n",
      "Step [7040/1000000], Loss: 3.03596, Vocab Loss: 2.71823, Token Loss: 1.84140\n",
      "Step [7050/1000000], Loss: 2.83106, Vocab Loss: 1.17550, Token Loss: 1.21814\n",
      "Step [7060/1000000], Loss: 3.19385, Vocab Loss: 2.29099, Token Loss: 1.53509\n",
      "Step [7070/1000000], Loss: 3.13899, Vocab Loss: 1.17722, Token Loss: 0.82473\n",
      "Step [7080/1000000], Loss: 3.15212, Vocab Loss: 2.20494, Token Loss: 1.46986\n",
      "Step [7090/1000000], Loss: 3.01219, Vocab Loss: 1.53118, Token Loss: 1.53586\n",
      "Step [7100/1000000], Loss: 3.04069, Vocab Loss: 1.69273, Token Loss: 1.63671\n",
      "Step [7110/1000000], Loss: 2.90928, Vocab Loss: 1.35449, Token Loss: 1.32252\n",
      "Step [7120/1000000], Loss: 2.76425, Vocab Loss: 1.44204, Token Loss: 1.51119\n",
      "Step [7130/1000000], Loss: 3.50383, Vocab Loss: 1.39797, Token Loss: 1.08910\n",
      "Step [7140/1000000], Loss: 2.70895, Vocab Loss: 2.17646, Token Loss: 1.31884\n",
      "Step [7150/1000000], Loss: 3.05600, Vocab Loss: 1.93330, Token Loss: 1.53664\n",
      "Step [7160/1000000], Loss: 3.04695, Vocab Loss: 0.96217, Token Loss: 1.04795\n",
      "Step [7170/1000000], Loss: 3.04633, Vocab Loss: 1.63417, Token Loss: 1.08959\n",
      "Step [7180/1000000], Loss: 3.10904, Vocab Loss: 3.04665, Token Loss: 1.65560\n",
      "Step [7190/1000000], Loss: 3.14267, Vocab Loss: 0.98442, Token Loss: 1.38420\n",
      "Step [7200/1000000], Loss: 2.78347, Vocab Loss: 1.18430, Token Loss: 1.05931\n",
      "Step [7210/1000000], Loss: 3.17933, Vocab Loss: 2.67174, Token Loss: 1.09468\n",
      "Step [7220/1000000], Loss: 3.41726, Vocab Loss: 2.38469, Token Loss: 1.51292\n",
      "Step [7230/1000000], Loss: 3.16434, Vocab Loss: 2.52137, Token Loss: 1.74430\n",
      "Step [7240/1000000], Loss: 2.71900, Vocab Loss: 1.80104, Token Loss: 1.09597\n",
      "Step [7250/1000000], Loss: 2.66084, Vocab Loss: 1.23285, Token Loss: 1.37702\n",
      "Step [7260/1000000], Loss: 3.12914, Vocab Loss: 0.91890, Token Loss: 1.17222\n",
      "Step [7270/1000000], Loss: 3.18138, Vocab Loss: 1.02698, Token Loss: 1.19712\n",
      "Step [7280/1000000], Loss: 3.28615, Vocab Loss: 1.93121, Token Loss: 1.49259\n",
      "Step [7290/1000000], Loss: 2.38905, Vocab Loss: 0.73662, Token Loss: 0.96278\n",
      "Step [7300/1000000], Loss: 3.24521, Vocab Loss: 2.39819, Token Loss: 1.41843\n",
      "Step [7310/1000000], Loss: 3.49999, Vocab Loss: 1.74236, Token Loss: 1.85103\n",
      "Step [7320/1000000], Loss: 3.32932, Vocab Loss: 2.59400, Token Loss: 1.55289\n",
      "Step [7330/1000000], Loss: 2.23508, Vocab Loss: 1.62628, Token Loss: 1.57550\n",
      "Step [7340/1000000], Loss: 3.16721, Vocab Loss: 1.56145, Token Loss: 1.16855\n",
      "Step [7350/1000000], Loss: 3.00020, Vocab Loss: 1.69287, Token Loss: 1.53190\n",
      "Step [7360/1000000], Loss: 2.51289, Vocab Loss: 0.94997, Token Loss: 1.58765\n",
      "Step [7370/1000000], Loss: 3.05878, Vocab Loss: 2.19551, Token Loss: 1.82402\n",
      "Step [7380/1000000], Loss: 3.06898, Vocab Loss: 1.10180, Token Loss: 1.19279\n",
      "Step [7390/1000000], Loss: 2.75099, Vocab Loss: 1.89166, Token Loss: 1.32139\n",
      "Step [7400/1000000], Loss: 2.54307, Vocab Loss: 0.87524, Token Loss: 1.40666\n",
      "Step [7410/1000000], Loss: 2.80741, Vocab Loss: 0.98163, Token Loss: 1.32527\n",
      "Step [7420/1000000], Loss: 2.70880, Vocab Loss: 1.78046, Token Loss: 1.24812\n",
      "Step [7430/1000000], Loss: 2.93038, Vocab Loss: 2.02400, Token Loss: 1.48956\n",
      "Step [7440/1000000], Loss: 3.00040, Vocab Loss: 1.50353, Token Loss: 1.37489\n",
      "Step [7450/1000000], Loss: 2.79387, Vocab Loss: 0.92883, Token Loss: 1.08869\n",
      "Step [7460/1000000], Loss: 3.17322, Vocab Loss: 1.25030, Token Loss: 1.44775\n",
      "Step [7470/1000000], Loss: 3.28131, Vocab Loss: 1.53804, Token Loss: 1.75876\n",
      "Step [7480/1000000], Loss: 2.84903, Vocab Loss: 1.06276, Token Loss: 1.17965\n",
      "Step [7490/1000000], Loss: 3.46085, Vocab Loss: 1.83513, Token Loss: 1.22133\n",
      "Step [7500/1000000], Loss: 3.06074, Vocab Loss: 1.20821, Token Loss: 0.67506\n",
      "Step [7510/1000000], Loss: 2.98285, Vocab Loss: 1.28094, Token Loss: 1.42772\n",
      "Step [7520/1000000], Loss: 3.02559, Vocab Loss: 1.46063, Token Loss: 1.46623\n",
      "Step [7530/1000000], Loss: 3.11936, Vocab Loss: 1.19260, Token Loss: 1.28923\n",
      "Step [7540/1000000], Loss: 3.21697, Vocab Loss: 1.15750, Token Loss: 1.20074\n",
      "Step [7550/1000000], Loss: 3.15753, Vocab Loss: 1.04985, Token Loss: 0.78299\n",
      "Step [7560/1000000], Loss: 3.26761, Vocab Loss: 1.91630, Token Loss: 1.70101\n",
      "Step [7570/1000000], Loss: 3.01000, Vocab Loss: 1.58106, Token Loss: 0.95547\n",
      "Step [7580/1000000], Loss: 2.56165, Vocab Loss: 0.88652, Token Loss: 1.00350\n",
      "Step [7590/1000000], Loss: 2.74028, Vocab Loss: 2.02780, Token Loss: 1.50516\n",
      "Step [7600/1000000], Loss: 2.65770, Vocab Loss: 1.00013, Token Loss: 0.98767\n",
      "Step [7610/1000000], Loss: 3.11470, Vocab Loss: 1.38771, Token Loss: 1.10237\n",
      "Step [7620/1000000], Loss: 2.91489, Vocab Loss: 1.71548, Token Loss: 1.36589\n",
      "Step [7630/1000000], Loss: 3.11816, Vocab Loss: 2.04938, Token Loss: 1.28633\n",
      "Step [7640/1000000], Loss: 2.88108, Vocab Loss: 2.28347, Token Loss: 1.18184\n",
      "Step [7650/1000000], Loss: 2.86877, Vocab Loss: 1.31471, Token Loss: 1.39056\n",
      "Step [7660/1000000], Loss: 2.81988, Vocab Loss: 0.99143, Token Loss: 1.58840\n",
      "Step [7670/1000000], Loss: 3.00610, Vocab Loss: 1.17434, Token Loss: 0.82365\n",
      "Step [7680/1000000], Loss: 3.08337, Vocab Loss: 2.08286, Token Loss: 1.64749\n",
      "Step [7690/1000000], Loss: 2.69018, Vocab Loss: 0.73780, Token Loss: 1.09257\n",
      "Step [7700/1000000], Loss: 2.51316, Vocab Loss: 1.03209, Token Loss: 0.94519\n",
      "Step [7710/1000000], Loss: 2.45884, Vocab Loss: 0.56480, Token Loss: 0.87375\n",
      "Step [7720/1000000], Loss: 3.27212, Vocab Loss: 0.97295, Token Loss: 1.23781\n",
      "Step [7730/1000000], Loss: 3.01957, Vocab Loss: 0.93228, Token Loss: 1.53300\n",
      "Step [7740/1000000], Loss: 2.92239, Vocab Loss: 0.96914, Token Loss: 1.34728\n",
      "Step [7750/1000000], Loss: 2.92417, Vocab Loss: 1.01614, Token Loss: 1.15368\n",
      "Step [7760/1000000], Loss: 2.64480, Vocab Loss: 1.11534, Token Loss: 0.78084\n",
      "Step [7770/1000000], Loss: 2.88504, Vocab Loss: 1.08699, Token Loss: 1.01024\n",
      "Step [7780/1000000], Loss: 2.77479, Vocab Loss: 0.98476, Token Loss: 1.07283\n",
      "Step [7790/1000000], Loss: 2.99508, Vocab Loss: 2.88552, Token Loss: 1.17555\n",
      "Step [7800/1000000], Loss: 2.97391, Vocab Loss: 2.43478, Token Loss: 1.35985\n",
      "Step [7810/1000000], Loss: 2.98108, Vocab Loss: 1.58838, Token Loss: 0.98183\n",
      "Step [7820/1000000], Loss: 2.59400, Vocab Loss: 1.36767, Token Loss: 1.19374\n",
      "Step [7830/1000000], Loss: 2.79353, Vocab Loss: 1.04137, Token Loss: 1.40667\n",
      "Step [7840/1000000], Loss: 2.59585, Vocab Loss: 0.90701, Token Loss: 0.90180\n",
      "Step [7850/1000000], Loss: 2.61349, Vocab Loss: 1.32517, Token Loss: 0.85831\n",
      "Step [7860/1000000], Loss: 2.74788, Vocab Loss: 0.92127, Token Loss: 1.45780\n",
      "Step [7870/1000000], Loss: 2.65079, Vocab Loss: 1.70710, Token Loss: 1.06319\n",
      "Step [7880/1000000], Loss: 2.85422, Vocab Loss: 1.04012, Token Loss: 1.05132\n",
      "Step [7890/1000000], Loss: 2.86385, Vocab Loss: 1.47942, Token Loss: 1.39008\n",
      "Step [7900/1000000], Loss: 2.56136, Vocab Loss: 0.94166, Token Loss: 0.88264\n",
      "Step [7910/1000000], Loss: 3.22857, Vocab Loss: 1.77185, Token Loss: 0.87769\n",
      "Step [7920/1000000], Loss: 3.01475, Vocab Loss: 2.56476, Token Loss: 1.51347\n",
      "Step [7930/1000000], Loss: 2.97709, Vocab Loss: 1.05602, Token Loss: 0.58569\n",
      "Step [7940/1000000], Loss: 2.47979, Vocab Loss: 0.84902, Token Loss: 0.94940\n",
      "Step [7950/1000000], Loss: 3.07301, Vocab Loss: 2.10784, Token Loss: 1.59682\n",
      "Step [7960/1000000], Loss: 2.96126, Vocab Loss: 1.14561, Token Loss: 1.18419\n",
      "Step [7970/1000000], Loss: 2.70335, Vocab Loss: 1.48704, Token Loss: 0.98911\n",
      "Step [7980/1000000], Loss: 2.73526, Vocab Loss: 0.88325, Token Loss: 1.15021\n",
      "Step [7990/1000000], Loss: 2.57597, Vocab Loss: 1.47758, Token Loss: 1.28156\n",
      "Step [8000/1000000], Loss: 2.56768, Vocab Loss: 1.48881, Token Loss: 1.26929\n",
      "Step [8010/1000000], Loss: 2.94194, Vocab Loss: 1.79976, Token Loss: 1.47869\n",
      "Step [8020/1000000], Loss: 2.95262, Vocab Loss: 1.96819, Token Loss: 1.42702\n",
      "Step [8030/1000000], Loss: 2.98178, Vocab Loss: 1.46353, Token Loss: 1.41781\n",
      "Step [8040/1000000], Loss: 2.91725, Vocab Loss: 1.89872, Token Loss: 1.58610\n",
      "Step [8050/1000000], Loss: 2.98220, Vocab Loss: 1.73778, Token Loss: 1.22917\n",
      "Step [8060/1000000], Loss: 2.78468, Vocab Loss: 1.26899, Token Loss: 1.28186\n",
      "Step [8070/1000000], Loss: 3.07657, Vocab Loss: 1.89334, Token Loss: 1.34771\n",
      "Step [8080/1000000], Loss: 2.71544, Vocab Loss: 1.78444, Token Loss: 1.56128\n",
      "Step [8090/1000000], Loss: 2.77389, Vocab Loss: 1.11352, Token Loss: 0.86921\n",
      "Step [8100/1000000], Loss: 3.05420, Vocab Loss: 1.79762, Token Loss: 1.64627\n",
      "Step [8110/1000000], Loss: 2.89844, Vocab Loss: 1.47538, Token Loss: 1.12262\n",
      "Step [8120/1000000], Loss: 2.98375, Vocab Loss: 1.26233, Token Loss: 1.01033\n",
      "Step [8130/1000000], Loss: 2.85605, Vocab Loss: 2.37222, Token Loss: 1.67416\n",
      "Step [8140/1000000], Loss: 2.44186, Vocab Loss: 1.37460, Token Loss: 1.02474\n",
      "Step [8150/1000000], Loss: 2.62091, Vocab Loss: 1.63651, Token Loss: 1.02060\n",
      "Step [8160/1000000], Loss: 2.77728, Vocab Loss: 1.17107, Token Loss: 0.95971\n",
      "Step [8170/1000000], Loss: 2.84587, Vocab Loss: 1.02239, Token Loss: 1.72664\n",
      "Step [8180/1000000], Loss: 2.94764, Vocab Loss: 2.49833, Token Loss: 1.29642\n",
      "Step [8190/1000000], Loss: 3.07339, Vocab Loss: 0.68887, Token Loss: 0.90193\n",
      "Step [8200/1000000], Loss: 3.02489, Vocab Loss: 2.62145, Token Loss: 1.72304\n",
      "Step [8210/1000000], Loss: 2.57973, Vocab Loss: 1.48659, Token Loss: 1.15230\n",
      "Step [8220/1000000], Loss: 2.62310, Vocab Loss: 2.44181, Token Loss: 1.52314\n",
      "Step [8230/1000000], Loss: 3.37525, Vocab Loss: 1.25185, Token Loss: 1.28516\n",
      "Step [8240/1000000], Loss: 2.78114, Vocab Loss: 1.49905, Token Loss: 1.36050\n",
      "Step [8250/1000000], Loss: 2.62648, Vocab Loss: 1.32819, Token Loss: 1.19122\n",
      "Step [8260/1000000], Loss: 2.36268, Vocab Loss: 0.78383, Token Loss: 0.79504\n",
      "Step [8270/1000000], Loss: 2.59995, Vocab Loss: 1.57717, Token Loss: 1.33898\n",
      "Step [8280/1000000], Loss: 2.81975, Vocab Loss: 0.81559, Token Loss: 0.79010\n",
      "Step [8290/1000000], Loss: 2.65874, Vocab Loss: 1.43442, Token Loss: 1.17684\n",
      "Step [8300/1000000], Loss: 2.75315, Vocab Loss: 0.91481, Token Loss: 0.77444\n",
      "Step [8310/1000000], Loss: 2.77610, Vocab Loss: 1.67606, Token Loss: 1.21097\n",
      "Step [8320/1000000], Loss: 2.91344, Vocab Loss: 1.14513, Token Loss: 1.11357\n",
      "Step [8330/1000000], Loss: 2.46203, Vocab Loss: 1.38704, Token Loss: 1.33083\n",
      "Step [8340/1000000], Loss: 2.58451, Vocab Loss: 0.54997, Token Loss: 0.56392\n",
      "Step [8350/1000000], Loss: 2.57284, Vocab Loss: 1.52282, Token Loss: 1.36698\n",
      "Step [8360/1000000], Loss: 2.68836, Vocab Loss: 1.19240, Token Loss: 1.32323\n",
      "Step [8370/1000000], Loss: 2.68589, Vocab Loss: 0.67589, Token Loss: 1.25468\n",
      "Step [8380/1000000], Loss: 2.39175, Vocab Loss: 1.73825, Token Loss: 0.90795\n",
      "Step [8390/1000000], Loss: 2.40493, Vocab Loss: 1.11439, Token Loss: 1.07409\n",
      "Step [8400/1000000], Loss: 2.89546, Vocab Loss: 1.77868, Token Loss: 1.65491\n",
      "Step [8410/1000000], Loss: 2.53009, Vocab Loss: 0.61444, Token Loss: 1.17070\n",
      "Step [8420/1000000], Loss: 2.92141, Vocab Loss: 1.66253, Token Loss: 1.29502\n",
      "Step [8430/1000000], Loss: 2.50338, Vocab Loss: 0.99196, Token Loss: 0.73859\n",
      "Step [8440/1000000], Loss: 2.61854, Vocab Loss: 2.64166, Token Loss: 1.54122\n",
      "Step [8450/1000000], Loss: 2.94012, Vocab Loss: 1.84194, Token Loss: 1.49293\n",
      "Step [8460/1000000], Loss: 2.76166, Vocab Loss: 0.86888, Token Loss: 0.93033\n",
      "Step [8470/1000000], Loss: 2.99576, Vocab Loss: 0.74610, Token Loss: 1.01565\n",
      "Step [8480/1000000], Loss: 3.17271, Vocab Loss: 1.50572, Token Loss: 1.08950\n",
      "Step [8490/1000000], Loss: 2.85925, Vocab Loss: 1.13452, Token Loss: 1.38262\n",
      "Step [8500/1000000], Loss: 2.41831, Vocab Loss: 0.54698, Token Loss: 0.65328\n",
      "Step [8510/1000000], Loss: 2.49139, Vocab Loss: 1.72088, Token Loss: 1.22198\n",
      "Step [8520/1000000], Loss: 2.52845, Vocab Loss: 1.20311, Token Loss: 0.81729\n",
      "Step [8530/1000000], Loss: 2.07799, Vocab Loss: 1.19978, Token Loss: 1.49683\n",
      "Step [8540/1000000], Loss: 2.35193, Vocab Loss: 0.76702, Token Loss: 0.86315\n",
      "Step [8550/1000000], Loss: 3.17501, Vocab Loss: 3.24529, Token Loss: 1.70111\n",
      "Step [8560/1000000], Loss: 2.31050, Vocab Loss: 0.88717, Token Loss: 0.80399\n",
      "Step [8570/1000000], Loss: 2.69107, Vocab Loss: 1.00911, Token Loss: 1.03740\n",
      "Step [8580/1000000], Loss: 2.57207, Vocab Loss: 2.48869, Token Loss: 1.43411\n",
      "Step [8590/1000000], Loss: 2.37061, Vocab Loss: 1.40548, Token Loss: 1.59543\n",
      "Step [8600/1000000], Loss: 2.71470, Vocab Loss: 1.13964, Token Loss: 1.19487\n",
      "Step [8610/1000000], Loss: 2.90390, Vocab Loss: 2.03151, Token Loss: 1.73198\n",
      "Step [8620/1000000], Loss: 3.12860, Vocab Loss: 1.46910, Token Loss: 1.42008\n",
      "Step [8630/1000000], Loss: 2.57433, Vocab Loss: 2.33868, Token Loss: 1.42260\n",
      "Step [8640/1000000], Loss: 2.84126, Vocab Loss: 0.95169, Token Loss: 1.12154\n",
      "Step [8650/1000000], Loss: 2.54028, Vocab Loss: 1.31174, Token Loss: 1.20908\n",
      "Step [8660/1000000], Loss: 2.43252, Vocab Loss: 1.47047, Token Loss: 0.96056\n",
      "Step [8670/1000000], Loss: 2.60436, Vocab Loss: 1.41328, Token Loss: 1.12670\n",
      "Step [8680/1000000], Loss: 2.53008, Vocab Loss: 0.81848, Token Loss: 0.97320\n",
      "Step [8690/1000000], Loss: 2.97907, Vocab Loss: 1.31702, Token Loss: 1.58458\n",
      "Step [8700/1000000], Loss: 2.90686, Vocab Loss: 1.21739, Token Loss: 1.55534\n",
      "Step [8710/1000000], Loss: 2.53484, Vocab Loss: 1.87491, Token Loss: 1.33318\n",
      "Step [8720/1000000], Loss: 2.81342, Vocab Loss: 0.74485, Token Loss: 1.54907\n",
      "Step [8730/1000000], Loss: 2.53526, Vocab Loss: 1.10065, Token Loss: 1.10722\n",
      "Step [8740/1000000], Loss: 2.50569, Vocab Loss: 0.86237, Token Loss: 0.89975\n",
      "Step [8750/1000000], Loss: 2.64187, Vocab Loss: 1.64893, Token Loss: 1.28622\n",
      "Step [8760/1000000], Loss: 2.58993, Vocab Loss: 1.32045, Token Loss: 1.22571\n",
      "Step [8770/1000000], Loss: 2.55385, Vocab Loss: 1.77602, Token Loss: 1.73596\n",
      "Step [8780/1000000], Loss: 2.53455, Vocab Loss: 1.10969, Token Loss: 1.41849\n",
      "Step [8790/1000000], Loss: 2.33329, Vocab Loss: 0.57528, Token Loss: 0.60259\n",
      "Step [8800/1000000], Loss: 2.89125, Vocab Loss: 2.02421, Token Loss: 1.03250\n",
      "Step [8810/1000000], Loss: 2.80399, Vocab Loss: 1.50917, Token Loss: 1.70310\n",
      "Step [8820/1000000], Loss: 2.91102, Vocab Loss: 1.85070, Token Loss: 1.64504\n",
      "Step [8830/1000000], Loss: 2.68606, Vocab Loss: 1.57244, Token Loss: 0.93441\n",
      "Step [8840/1000000], Loss: 2.62001, Vocab Loss: 0.83031, Token Loss: 1.13885\n",
      "Step [8850/1000000], Loss: 2.98388, Vocab Loss: 2.33764, Token Loss: 1.28942\n",
      "Step [8860/1000000], Loss: 2.53600, Vocab Loss: 1.16548, Token Loss: 1.33769\n",
      "Step [8870/1000000], Loss: 2.64360, Vocab Loss: 1.25209, Token Loss: 1.34346\n",
      "Step [8880/1000000], Loss: 2.27701, Vocab Loss: 0.91614, Token Loss: 1.35622\n",
      "Step [8890/1000000], Loss: 2.70484, Vocab Loss: 1.13705, Token Loss: 1.41949\n",
      "Step [8900/1000000], Loss: 2.55945, Vocab Loss: 2.42294, Token Loss: 1.38290\n",
      "Step [8910/1000000], Loss: 2.25335, Vocab Loss: 1.33337, Token Loss: 1.09132\n",
      "Step [8920/1000000], Loss: 2.56108, Vocab Loss: 1.43443, Token Loss: 1.07866\n",
      "Step [8930/1000000], Loss: 2.81778, Vocab Loss: 1.20549, Token Loss: 1.28934\n",
      "Step [8940/1000000], Loss: 2.70865, Vocab Loss: 2.35899, Token Loss: 1.20841\n",
      "Step [8950/1000000], Loss: 2.32372, Vocab Loss: 1.63375, Token Loss: 1.48508\n",
      "Step [8960/1000000], Loss: 2.72750, Vocab Loss: 2.69166, Token Loss: 1.40223\n",
      "Step [8970/1000000], Loss: 2.80014, Vocab Loss: 1.18538, Token Loss: 0.98971\n",
      "Step [8980/1000000], Loss: 2.68125, Vocab Loss: 2.20206, Token Loss: 1.66218\n",
      "Step [8990/1000000], Loss: 2.60098, Vocab Loss: 1.90386, Token Loss: 1.47696\n",
      "Step [9000/1000000], Loss: 2.45728, Vocab Loss: 0.52100, Token Loss: 1.04105\n",
      "Step [9010/1000000], Loss: 2.52968, Vocab Loss: 1.29774, Token Loss: 1.21102\n",
      "Step [9020/1000000], Loss: 2.97763, Vocab Loss: 1.10921, Token Loss: 1.11781\n",
      "Step [9030/1000000], Loss: 2.19930, Vocab Loss: 0.65218, Token Loss: 1.26035\n",
      "Step [9040/1000000], Loss: 2.43522, Vocab Loss: 1.50195, Token Loss: 0.92309\n",
      "Step [9050/1000000], Loss: 2.75796, Vocab Loss: 2.15910, Token Loss: 1.48291\n",
      "Step [9060/1000000], Loss: 2.55336, Vocab Loss: 1.40035, Token Loss: 1.17312\n",
      "Step [9070/1000000], Loss: 2.83184, Vocab Loss: 2.47264, Token Loss: 1.46209\n",
      "Step [9080/1000000], Loss: 2.20871, Vocab Loss: 1.97656, Token Loss: 1.44710\n",
      "Step [9090/1000000], Loss: 2.32823, Vocab Loss: 1.54539, Token Loss: 1.44967\n",
      "Step [9100/1000000], Loss: 2.42036, Vocab Loss: 1.35115, Token Loss: 1.10500\n",
      "Step [9110/1000000], Loss: 2.29813, Vocab Loss: 1.94224, Token Loss: 1.51098\n",
      "Step [9120/1000000], Loss: 2.45051, Vocab Loss: 0.95502, Token Loss: 0.89032\n",
      "Step [9130/1000000], Loss: 2.28664, Vocab Loss: 1.33766, Token Loss: 1.40378\n",
      "Step [9140/1000000], Loss: 2.40820, Vocab Loss: 0.57451, Token Loss: 1.14054\n",
      "Step [9150/1000000], Loss: 2.46032, Vocab Loss: 0.96607, Token Loss: 1.73148\n",
      "Step [9160/1000000], Loss: 2.37788, Vocab Loss: 1.65716, Token Loss: 1.32714\n",
      "Step [9170/1000000], Loss: 2.42106, Vocab Loss: 1.37868, Token Loss: 0.91708\n",
      "Step [9180/1000000], Loss: 2.59553, Vocab Loss: 1.05837, Token Loss: 0.91475\n",
      "Step [9190/1000000], Loss: 2.77329, Vocab Loss: 1.22542, Token Loss: 1.29029\n",
      "Step [9200/1000000], Loss: 2.74717, Vocab Loss: 1.95081, Token Loss: 1.13500\n",
      "Step [9210/1000000], Loss: 2.87500, Vocab Loss: 2.84147, Token Loss: 1.66756\n",
      "Step [9220/1000000], Loss: 3.04153, Vocab Loss: 1.83807, Token Loss: 1.35930\n",
      "Step [9230/1000000], Loss: 2.26490, Vocab Loss: 1.72244, Token Loss: 1.19180\n",
      "Step [9240/1000000], Loss: 2.65612, Vocab Loss: 1.21440, Token Loss: 1.03442\n",
      "Step [9250/1000000], Loss: 2.29282, Vocab Loss: 0.85125, Token Loss: 1.12957\n",
      "Step [9260/1000000], Loss: 2.11079, Vocab Loss: 1.39194, Token Loss: 1.17009\n",
      "Step [9270/1000000], Loss: 2.50715, Vocab Loss: 0.72449, Token Loss: 0.25184\n",
      "Step [9280/1000000], Loss: 2.40753, Vocab Loss: 0.81830, Token Loss: 0.88109\n",
      "Step [9290/1000000], Loss: 2.35402, Vocab Loss: 1.60398, Token Loss: 1.26868\n",
      "Step [9300/1000000], Loss: 2.25937, Vocab Loss: 1.43969, Token Loss: 1.11588\n",
      "Step [9310/1000000], Loss: 2.51158, Vocab Loss: 1.94312, Token Loss: 1.24227\n",
      "Step [9320/1000000], Loss: 2.01214, Vocab Loss: 0.63711, Token Loss: 0.66025\n",
      "Step [9330/1000000], Loss: 2.43196, Vocab Loss: 1.42472, Token Loss: 0.74651\n",
      "Step [9340/1000000], Loss: 2.82282, Vocab Loss: 1.18459, Token Loss: 1.00140\n",
      "Step [9350/1000000], Loss: 2.67336, Vocab Loss: 2.30849, Token Loss: 1.45769\n",
      "Step [9360/1000000], Loss: 2.28949, Vocab Loss: 0.57050, Token Loss: 0.75923\n",
      "Step [9370/1000000], Loss: 2.88652, Vocab Loss: 0.64276, Token Loss: 0.67932\n",
      "Step [9380/1000000], Loss: 2.26593, Vocab Loss: 1.11772, Token Loss: 1.45703\n",
      "Step [9390/1000000], Loss: 2.28320, Vocab Loss: 0.91752, Token Loss: 0.99645\n",
      "Step [9400/1000000], Loss: 2.42181, Vocab Loss: 0.43088, Token Loss: 0.25069\n",
      "Step [9410/1000000], Loss: 2.36257, Vocab Loss: 1.43538, Token Loss: 0.87668\n",
      "Step [9420/1000000], Loss: 2.38618, Vocab Loss: 1.47231, Token Loss: 0.84642\n",
      "Step [9430/1000000], Loss: 2.62280, Vocab Loss: 1.15332, Token Loss: 1.21327\n",
      "Step [9440/1000000], Loss: 2.63833, Vocab Loss: 1.94033, Token Loss: 1.17010\n",
      "Step [9450/1000000], Loss: 2.58020, Vocab Loss: 2.41865, Token Loss: 1.29121\n",
      "Step [9460/1000000], Loss: 1.90933, Vocab Loss: 0.81449, Token Loss: 1.02034\n",
      "Step [9470/1000000], Loss: 2.35716, Vocab Loss: 1.97565, Token Loss: 0.81593\n",
      "Step [9480/1000000], Loss: 2.39881, Vocab Loss: 1.68775, Token Loss: 1.42738\n",
      "Step [9490/1000000], Loss: 2.71132, Vocab Loss: 1.05082, Token Loss: 1.04727\n",
      "Step [9500/1000000], Loss: 2.63102, Vocab Loss: 1.75648, Token Loss: 1.26555\n",
      "Step [9510/1000000], Loss: 2.34182, Vocab Loss: 1.51900, Token Loss: 1.44219\n",
      "Step [9520/1000000], Loss: 2.49395, Vocab Loss: 1.78087, Token Loss: 1.28275\n",
      "Step [9530/1000000], Loss: 2.29386, Vocab Loss: 0.59087, Token Loss: 0.52791\n",
      "Step [9540/1000000], Loss: 1.88404, Vocab Loss: 1.30115, Token Loss: 1.09567\n",
      "Step [9550/1000000], Loss: 2.16560, Vocab Loss: 1.24275, Token Loss: 0.66709\n",
      "Step [9560/1000000], Loss: 2.23028, Vocab Loss: 1.39553, Token Loss: 1.26602\n",
      "Step [9570/1000000], Loss: 2.70149, Vocab Loss: 1.03803, Token Loss: 0.96926\n",
      "Step [9580/1000000], Loss: 2.55447, Vocab Loss: 1.78221, Token Loss: 1.20939\n",
      "Step [9590/1000000], Loss: 2.50573, Vocab Loss: 1.47857, Token Loss: 0.95149\n",
      "Step [9600/1000000], Loss: 2.71199, Vocab Loss: 1.12962, Token Loss: 1.12012\n",
      "Step [9610/1000000], Loss: 2.30893, Vocab Loss: 1.14175, Token Loss: 1.07207\n",
      "Step [9620/1000000], Loss: 2.48776, Vocab Loss: 1.40793, Token Loss: 0.89619\n",
      "Step [9630/1000000], Loss: 2.51027, Vocab Loss: 1.32902, Token Loss: 1.06081\n",
      "Step [9640/1000000], Loss: 2.45779, Vocab Loss: 1.29255, Token Loss: 1.02134\n",
      "Step [9650/1000000], Loss: 2.67145, Vocab Loss: 0.78738, Token Loss: 0.92828\n",
      "Step [9660/1000000], Loss: 2.47447, Vocab Loss: 2.14721, Token Loss: 1.66525\n",
      "Step [9670/1000000], Loss: 2.82204, Vocab Loss: 1.66582, Token Loss: 1.41060\n",
      "Step [9680/1000000], Loss: 2.35873, Vocab Loss: 1.19729, Token Loss: 1.02154\n",
      "Step [9690/1000000], Loss: 2.70003, Vocab Loss: 1.20217, Token Loss: 1.17440\n",
      "Step [9700/1000000], Loss: 2.32344, Vocab Loss: 0.83912, Token Loss: 0.90340\n",
      "Step [9710/1000000], Loss: 2.23941, Vocab Loss: 1.24517, Token Loss: 1.00899\n",
      "Step [9720/1000000], Loss: 2.37546, Vocab Loss: 0.53955, Token Loss: 1.10925\n",
      "Step [9730/1000000], Loss: 2.40603, Vocab Loss: 0.99827, Token Loss: 1.01785\n",
      "Step [9740/1000000], Loss: 2.06520, Vocab Loss: 0.89674, Token Loss: 0.99468\n",
      "Step [9750/1000000], Loss: 2.52000, Vocab Loss: 1.90270, Token Loss: 1.30214\n",
      "Step [9760/1000000], Loss: 2.41957, Vocab Loss: 1.25596, Token Loss: 1.09064\n",
      "Step [9770/1000000], Loss: 2.25333, Vocab Loss: 1.17071, Token Loss: 0.84418\n",
      "Step [9780/1000000], Loss: 2.65533, Vocab Loss: 1.08250, Token Loss: 1.19949\n",
      "Step [9790/1000000], Loss: 2.59233, Vocab Loss: 1.00989, Token Loss: 1.13701\n",
      "Step [9800/1000000], Loss: 2.61943, Vocab Loss: 0.97035, Token Loss: 1.26677\n",
      "Step [9810/1000000], Loss: 2.73628, Vocab Loss: 1.43512, Token Loss: 1.29669\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m33389\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/BERT/lib/python3.8/site-packages/accelerate/launchers.py:266\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 266\u001b[0m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _s2s_pred, _text_input, _text_length, _masked_indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tokens_pred, labels, input_lengths, masked_indices):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_masked_indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m         _text_input \u001b[38;5;241m=\u001b[39m \u001b[43m_text_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m_text_length\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_masked_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     88\u001b[0m         loss_tmp \u001b[38;5;241m=\u001b[39m criterion(_s2s_pred[:_text_length][_masked_indices], \n\u001b[1;32m     89\u001b[0m                                     _text_input[:_text_length]) \n\u001b[1;32m     90\u001b[0m         loss_token \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_tmp\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "while True:\n",
    "    notebook_launcher(train, args=(), num_processes=1, use_port=33389)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf4988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
